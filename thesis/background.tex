%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Background
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Background}
  \label{Background}

\todo[inline]{TODO}

\section{Reinforcement Learning}
Reinforcement Learning (RL) is a machine learning method that, in contrast to supervised and not-supervised methods, does need labeled data for learning. Instead, it explores in an environment the possible actions and aims to maximize the long-term reward. Formally, at each time step $t$, the environment is in a state $s_t$ that is observed by the agent via an observation $o_t$. This observation may not include the complete state. Based on this observation, the agent takes an action $a_t$, which determines the next state $s_{t+1}$ of the environment. 


\subsection{Soft Actor-Critic}
\begin{algorithm}
\caption{Soft Actor-Critic \cite{haarnoja2018soft}}
\label{alg:sac-update}
\small\textbf{Input:}   $\theta_1,\theta_2,\phi$ \Comment{Initial parameters}
\begin{algorithmic}
\State $\bar{\theta}_1 \gets \theta_1, \bar{\theta}_2 \gets \theta_2$ \Comment{Initialize target network weights}
\State $\mathcal{D} \gets \emptyset$ \Comment{Initialize an empty replay pool}
\For{each iteration}
	\For{each environment step}
		\State $\mathbf{a}_t \sim \pi_{\phi}(\mathbf{a}_t|\mathbf{s}_t)$ \Comment{Sample action from the policy}
		\State $\mathbf{s}_{t+1} \sim p(\mathbf{s}_{t+1}|\mathbf{a}_t)$ \Comment{Sample transition from the environment}
		\State $\mathcal{D} \gets \mathcal{D} \cup \{(\mathbf{s}_t,\mathbf{a}_t,r(\mathbf{s}_t,\mathbf{a}_t),\mathbf{s}_{t+1})\}$ \Comment{Store the transition in the replay pool}
	\EndFor
	\For{each gradient step}
		\State $\theta_i \gets \theta_i - \lambda_Q \hat{\triangledown}_{\theta_i}J_Q(\theta_i)$ for $i \in \{1,2\}$ \Comment{Update Q-function parameters}
		\State $\phi \gets \phi - \lambda _{\pi} \hat{\triangledown}_{\phi}J_{\pi}(\phi)$ \Comment{Update policy weights}
		\State $\alpha \gets \alpha - \lambda  \hat{\triangledown}_{\alpha}J(\alpha)$ \Comment{Adjust temperature}
		\State $\bar{\theta}_i \gets \tau\theta_i + (1-\tau)\bar{\theta}_i$ for $i \in \{1,2\}$ \Comment{Update target network weights}
	\EndFor
\EndFor
\end{algorithmic}
\textbf{Output:}   $\theta_1,\theta_2,\phi$ \Comment{Optimized parameters}
\end{algorithm}


\section{Social Learning}
\subsection{Decision Biasing}
\begin{algorithm}
\caption{\textcolor{red}{Mine DB2}}
\label{alg:sac-update}
\small\textbf{Input:}   $\theta_1,\theta_2,\phi$ \Comment{Initial parameters}
\begin{algorithmic}
\State $\bar{\theta}_1 \gets \theta_1, \bar{\theta}_2 \gets \theta_2$ \Comment{Initialize target network weights}
\State $\mathcal{D} \gets \emptyset$ \Comment{Initialize an empty replay pool}
\For{each iteration}
	\For{each environment step}
		\State $\mathbf{a}_t \sim \pi_{\phi}(\mathbf{a}_t|\mathbf{s}_t)$ \Comment{Sample action from the policy}
		\State $\mathbf{s}_{t+1} \sim p(\mathbf{s}_{t+1}|\mathbf{a}_t)$ \Comment{Sample transition from the environment}
		\State $\mathcal{D} \gets \mathcal{D} \cup \{(\mathbf{s}_t,\mathbf{a}_t,r(\mathbf{s}_t,\mathbf{a}_t),\mathbf{s}_{t+1})\}$ \Comment{Store the transition in the replay pool}
	\EndFor
	\For{each gradient step}
		\State $\theta_i \gets \theta_i - \lambda_Q \hat{\triangledown}_{\theta_i}J_Q(\theta_i)$ for $i \in \{1,2\}$ \Comment{Update Q-function parameters}
		\State $\phi \gets \phi - \lambda _{\pi} \hat{\triangledown}_{\phi}J_{\pi}(\phi)$ \Comment{Update policy weights}
		\State $\alpha \gets \alpha - \lambda  \hat{\triangledown}_{\alpha}J(\alpha)$ \Comment{Adjust temperature}
		\State $\bar{\theta}_i \gets \tau\theta_i + (1-\tau)\bar{\theta}_i$ for $i \in \{1,2\}$ \Comment{Update target network weights}
		\If{agent not a demonstrator}
			\For{each demonstrator $d$}
				\State $\mathbf{a}_t^d \sim \pi_{\phi}^d(\mathbf{a}_t^d|\mathbf{s}_t)$
				\State $q_t^d = \min_i J_Q(\theta_i)$
				\State $q_t^d = q_t^d + \lambda_d(1-q_t^d)$
				\State $\phi \gets \phi - \lambda _{\pi} \hat{\triangledown}_{\phi}J_{\pi}(\phi)$ \Comment{halt loss mit diesen q-values berechnen und dann backward step}
			\EndFor
		\EndIf
	\EndFor
\EndFor
\end{algorithmic}
\textbf{Output:}   $\theta_1,\theta_2,\phi$ \Comment{Optimized parameters}
\end{algorithm}


