%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Background
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reinforcement Learning}
\label{sec:reinforcement-learning}
As defined by Sutton and Barto \cite{sutton2018reinforcement}, RL is a machine learning method that aims to choose actions to maximize a numerical reward. RL agents learn these actions through a trial-and-error approach based on the received rewards. Also, actions may, besides the immediate reward, influence the long-term reward. In contrast to supervised methods, it does not need labeled data for learning and does not try to discover inherent structure in data as unsupervised methods.

\begin{figure}[htb]
     \center
     \includegraphics[width=\textwidth]{figures/rl_overview.pdf}
  \caption[Reinforcement Learning Overview]{In RL, the agent observes the state of the environment $s_t$ at timestep $t$ via the observation $o_t$. The policy $\pi$ determines the agent's action $a_t$, which in turn determines the next state $s_{t+1}$ of the environment and the environment sends an reward $r_t$ to the agent. The policy of the agent is trained using the observation, the action, and the reward,}
  \label{fig:rl_overview}
\end{figure}

\noindent
Figure \ref{fig:rl_overview} shows the general RL approach. RL problems are often formulated as finite MDPs, 
%defined by a tuple $(\mathcal{S}, \mathcal{A}, p, r)$, 
where the agent interacts with the environment over a series of time steps. At each time step $t$, the agent observes the current state of the environment, denoted as $s_t \in \mathcal{S}$. Based on this state, the agent selects an action $a_t \in \mathcal{A}$. We assume, that the set of possible actions $\mathcal{A}$ is the same in all states. As a consequence of the chosen action, the agent receives at the next time step a numerical reward $r_{t+1}\in\mathcal{R} \subset \mathbb{R}$ and a new state $s_{t+1}$.

\todo[inline]{discrete spaces, markov property}

The dynamics of the MDP are captured by a function $p: \mathcal{S}\times\mathcal{R}\times\mathcal{S}\times\mathcal{A} \rightarrow [0,1]$. This function represents the probability of transitioning from state $s\in\mathcal{S}$ to state $s'\in\mathcal{S}$ and receiving reward $r\in\mathcal{R}$ when the action $a\in\mathcal{A}$ is taken:
\begin{equation}
p(s', r \mid s,a)=P(s_t=s', r_t=r \mid s_{t-1}=s, a_{t-1}=a)
\end{equation}
Using the dynamics function $p$, we can calculate additional quantities of interest. The state-transition probability defined as function $p: \mathcal{S}\times\mathcal{S}\times\mathcal{A} \rightarrow [0,1]$ is the probability of transitioning to state $s'$ when action $a$ is taken, given the current state is $s$: 
\begin{equation}
p(s'\mid s,a)=P(s_t=s'\mid s_{t-1}=s, a_{t-1}=a) = \sum_{r\in\mathcal{R}}p(s',r \mid s,a)
\end{equation}
Furthermore, we can calculate the expected reward for a given state-action pair $(s,a)$ as function $r: \mathcal{S}\times\mathcal{A}\rightarrow \mathbb{R}$: 
\begin{equation}
r(s,a)=\mathbb{E}[r_t\mid s_{t-1}=s,a_{t-1}=a]=\sum_{r\in\mathcal{R}}r\sum_{s'\in\mathcal{S}}p(s',r\mid s,a)
\end{equation}

\begin{itemize}
\item Goal: maximize expected discounted return $G_t = R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+... = \sum_{k=0}^{\infty}\gamma^kR_{t+k+1}$ with discount rate $0 \leq \gamma \leq 1$
\end{itemize} 


The policy $\pi: \mathcal{S}=p(a_t|s_t)$ determines the action, which may also be deterministic. Also, reacting to the received action, the environment sends the agent a reward signal $r_t$. The agent's policy is trained to maximize the discounted total reward. 


\todo[inline]{TODO Continuous action - 335, continuous state ---$>$ function approximation}



\subsection{Soft Actor-Critic}
\label{sec:SAC}
\todo[inline]{TODO Continuous spaces, argmax}
Soft Actor-Critic (SAC) is a model-free RL algorithm based on the maximum entropy RL framework, which aims to maximize the maximum entropy objective:
\begin{equation}
	\pi^*=argmax_{\pi}\sum_t \mathbb{E}_{(s_t,a_t)\sim\rho_{\pi}} \left [r(s_t,a_t)+\alpha \mathcal{H}(\pi(\cdot[s_t) \right ]
\end{equation}
The first part of this objective is equvialent to the classical maiximizing total expected reward approach and the second term adds the maximum entropy of the policy. The trade-off between these two is controlled by the temperature parameter $\alpha$. As $\alpha \rightarrow 0$ this objective is the same as in standard RL. The entropy term introduces stochasticity to the policy and thus favors exploration. 

SAC can be derived form the Soft Policy Iteration, an algorithm that alternates between the policy evaluation step, where the soft Q-function of the current policy $\pi$ is calculated, and the policy iteration step, where the policy is improved in terms of its soft Q-value. To ensure the tractability of the obtained policy, it is restricted to a set of possible policies $\Pi$ using the information projection in terms of the Kullback-Leibler (KL) divergence. For tabular setting the convergence of the Soft Policy Iteration towards the optimal policy $\pi^* \in \Pi$ in terms of the maximum entropy RL is guaranteed.

However, for continuous space and action dimensions, functional approximators are needed for the Q-value function $Q_{\theta}(s_t,a_t)$and the policy $\pi_{\phi}(a_t|s_t)$ with parameters $\theta$ and $\phi$. For these, iterating the Soft Policy Iteration algorithm until convergence is computationally intractable. Hence the authors introduced SAC, alternating between optimizing the networks with stochastic gradient descent. The parameters of the soft Q-value functions are minimized using the soft Bellman residual using a target network with parameters $\bar{\theta}$ to stabilize training:
\begin{equation}
	J_Q(\theta)=\mathbb{E}_{(s_t,a_t)\sim\mathcal{D}}\left[\frac{1}{2}(Q_\theta(s_t,a_t)-(r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim p}\left[V_{\bar{\theta}}(s_{t+1}) \right ]))^2 \right].
\end{equation}
The update rule given by stochastic gradient decent is given by 
\begin{equation} \small
	\hat{\triangledown}_{\theta}J_Q(\theta)=\triangledown_{\theta} Q_{\theta}(a_t,s_t)(Q_{\theta}(s_t,a_t)-(r(s_t,a_t)+\gamma(Q_{\bar{\theta}}(s_{t+1},a_{t+1})-\alpha \log (\pi_\phi(a_{t+1}|s_{t+1})))).
\end{equation}
The objective function for updating the policy is obtained by minimizing the expected KL divergence: 
\begin{equation}
	J_{\pi}(\phi)=\mathbb{E}_{s_t\sim\mathcal{D},\epsilon_t\sim\mathcal{N}}\left [\alpha \log \pi_{\phi}(f_{\phi}(\epsilon_t;s_t)|s_t)-Q_{\theta}(s_t, f_{\phi}(\epsilon_t;s_t)) \right ],
\end{equation}
where the policy is reparametrizied by a neural network $a_t=f_{\phi}(\epsilon_t;s_t)$ with input nois $\epsilon_t$ to apply the reparametrization trick \todo[inline]{Source}. Then the update rule is 
\begin{equation}
	\hat{\triangledown}_{\phi}J_{\pi}(\phi)=\triangledown_{\phi}\alpha\log (\pi_{\phi}(a_t|s_t))+(\triangledown_{a_t}\alpha \log (\pi_{\phi}(a_t|s_t))-\triangledown_{a_t}Q(s_t,a_t))\triangledown_{\phi}f_{\phi}(\epsilon_t;s_t).
\end{equation}

The temperature parameters $\alpha$ is important for the success of SAC and needs to be tuned for each task individually. Even during training of one single task it would be advantageous if $\alpha$ would be adjustable: In regions, where the policy is very sure about the best action, the entropy should be rather small and thus the policy deterministic. In regions, where the policy is unsure, exploration should be enhanced. Thus the authors presented a solution for adjusting the temperature parameter automatically and proved is theoretical optimalicy in the tabular setting. For this, the new objective considers the target minimum entropy $\mathcal{H}$ in a constraint to the classical maximum reward approach:
\begin{equation}
	\max_{\pi_{0:T}}\mathbb{E}_{\rho_{\pi}}\left[\sum_{t=0}^Tr(s_t,a_t) \right ] \textbf{ s.t. } \mathbb{E}_{(s_t,a_t)\sim \rho_{\pi}}\left [-\log(\pi_t(a_t|s_t)) \right ] \geq \mathcal{H}~\forall t
\end{equation}
Haarnoja et al. reformulated this objective as a dual problem, where $\alpha_T$ is the dual variable. After solving for the optimal soft Q-value function $Q^*_t$ and the optimal policy $\pi^*_t$, the optimal temperature parameter at time step $t$ is given by
\begin{equation}\label{eq:obj-alpha}
	\alpha^*_t = argmin_{\alpha_t} \mathbb{E}_{a_t \sim \pi^*_t}\left [-\alpha_t \log \pi^*_t(a_t|s_t; \alpha_t)-\alpha_t \bar{\mathcal{H}} \right ].
\end{equation}
For the final SAC algorithm, as given in Algorithm \ref{alg:sac-update}, the authors propose to use two soft Q-function approximators trained independently to lower the positive bias introduced in the policy improvement step. The minimum value of these is used for updating both Q-function approximators as well as the policy. The temperature parameter $\alpha$ is learned by computing the gradient of the objective given in Equation \ref{eq:obj-alpha}, where $Q_t$ and $\pi_t$ are assumed to be optimal for the current time step.


\begin{algorithm}
\caption{Soft Actor-Critic \cite{haarnoja2018soft}}
\label{alg:sac-update}
\small\textbf{Input:}   $\theta_1,\theta_2,\phi$ \Comment{Initial parameters}
\begin{algorithmic}
\State $\bar{\theta}_1 \gets \theta_1, \bar{\theta}_2 \gets \theta_2$ \Comment{Initialize target network weights}
\State $\mathcal{D} \gets \emptyset$ \Comment{Initialize an empty replay pool}
\For{each iteration}
	\For{each environment step}
		\State $\mathbf{a}_t \sim \pi_{\phi}(\mathbf{a}_t|\mathbf{s}_t)$ \Comment{Sample action from the policy}
		\State $\mathbf{s}_{t+1} \sim p(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)$ \Comment{Sample transition from the environment}
		\State $\mathcal{D} \gets \mathcal{D} \cup \{(\mathbf{s}_t,\mathbf{a}_t,r(\mathbf{s}_t,\mathbf{a}_t),\mathbf{s}_{t+1})\}$ \Comment{Store the transition in the replay pool}
	\EndFor
	\For{each gradient step}
		\State $\theta_i \gets \theta_i - \lambda_Q \hat{\triangledown}_{\theta_i}J_Q(\theta_i)$ for $i \in \{1,2\}$ \Comment{Update Q-function parameters}
		\State $\phi \gets \phi - \lambda _{\pi} \hat{\triangledown}_{\phi}J_{\pi}(\phi)$ \Comment{Update policy weights}
		\State $\alpha \gets \alpha - \lambda  \hat{\triangledown}_{\alpha}J(\alpha)$ \Comment{Adjust temperature}
		\State $\bar{\theta}_i \gets \tau\theta_i + (1-\tau)\bar{\theta}_i$ for $i \in \{1,2\}$ \Comment{Update target network weights}
	\EndFor
\EndFor
\end{algorithmic}
\textbf{Output:}   $\theta_1,\theta_2,\phi$ \Comment{Optimized parameters}
\end{algorithm}


\section{Social Learning}
\label{sec:background-social-learning}

Demonstrator transitions in replay buffer \cite{vecerik2017leveraging}

\cite{hester2018deep}

Imitation learning (soll ja nicht unbedingt imitieren, sondern 'miteinander arbeiten', kein human demonstrator)

\subsection{Decision Biasing}
\begin{algorithm}
\caption{\textcolor{red}{Mine DB2}}
\label{alg:sac-update-db2}
\small\textbf{Input:}   $\theta_1,\theta_2,\phi$ \Comment{Initial parameters}
\begin{algorithmic}
\State $\bar{\theta}_1 \gets \theta_1, \bar{\theta}_2 \gets \theta_2$ \Comment{Initialize target network weights}
\State $\mathcal{D} \gets \emptyset$ \Comment{Initialize an empty replay pool}
\For{each iteration}
	\For{each environment step}
		\State $\mathbf{a}_t \sim \pi_{\phi}(\mathbf{a}_t|\mathbf{s}_t)$ \Comment{Sample action from the policy}
		\State $\mathbf{s}_{t+1} \sim p(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)$ \Comment{Sample transition from the environment}
		\State $\mathcal{D} \gets \mathcal{D} \cup \{(\mathbf{s}_t,\mathbf{a}_t,r(\mathbf{s}_t,\mathbf{a}_t),\mathbf{s}_{t+1})\}$ \Comment{Store the transition in the replay pool}
	\EndFor
	\For{each gradient step}
		\State $\theta_i \gets \theta_i - \lambda_Q \hat{\triangledown}_{\theta_i}J_Q(\theta_i)$ for $i \in \{1,2\}$ \Comment{Update Q-function parameters}
		\State $\phi \gets \phi - \lambda _{\pi} \hat{\triangledown}_{\phi}J_{\pi}(\phi)$ \Comment{Update policy weights}
		\State $\alpha \gets \alpha - \lambda  \hat{\triangledown}_{\alpha}J(\alpha)$ \Comment{Adjust temperature}
		\State $\bar{\theta}_i \gets \tau\theta_i + (1-\tau)\bar{\theta}_i$ for $i \in \{1,2\}$ \Comment{Update target network weights}
		\If{agent not a demonstrator}
			\For{each demonstrator $d$}
				\State $\mathbf{a}_t^d \sim \pi_{\phi}^d(\mathbf{a}_t^d|\mathbf{s}_t)$
				\State $q_t^d = \min_i J_Q(\theta_i)$
				\State $q_t^d = q_t^d + \lambda_d(1-q_t^d)$
				\State $\phi \gets \phi - \lambda _{\pi} \hat{\triangledown}_{\phi}J_{\pi}(\phi)$ \Comment{halt loss mit diesen q-values berechnen und dann backward step}
			\EndFor
		\EndIf
	\EndFor
\EndFor
\end{algorithmic}
\textbf{Output:}   $\theta_1,\theta_2,\phi$ \Comment{Optimized parameters}
\end{algorithm}


