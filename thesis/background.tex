%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Background
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Theoretical Foundations}
\label{chap:background}
This chapter provides the necessary background knowledge to comprehend our approach to the presented problem. First, we introduce the fundamental concepts of RL and the SAC algorithm. Then, we explain the algorithm-independent social methods imitation learning and DB and present the MARLISA algorithm.

\section{Reinforcement Learning Paradigms}
\label{sec:reinforcement-learning}
RL is an agent-based approach within artificial intelligence in which an agent acts in an environment, observes the current state and performs actions to maximize a cumulative reward over time. This chapter describes the basic mathematical principles of RL. Traditional RL methods often struggle with the limitations of discrete action spaces. However, the SAC algorithm overcomes this limitation through continuous action selection. We explain the SAC algorithm and elaborate on its mathematical framework.

\subsection{Fundamentals of Reinforcement Learning}
As defined by Sutton and Barto \cite{sutton2018reinforcement}, RL is a machine learning method that aims to choose actions to maximize a numerical reward. RL agents learn these actions through a trial-and-error approach based on the rewards they receive. Actions can affect both immediate rewards and long-term rewards. In contrast to supervised methods, RL does not need labeled data for learning and does not try to discover inherent structure in data as unsupervised methods.

{
\renewcommand{\baselinestretch}{0.9} 
\normalsize
\begin{figure}[htb]
  \center
  \scalebox{.8}{%
  \begin{overpic}[width=\textwidth, trim={0 1.7cm 0 0.3cm},clip]{figures/rl_overview_blank.pdf}%
	\put(455,415){Agent}%
	\put(450,351){Policy $\pi$}%
	\put(503,290){\small Policy update}%
	\put(409,223){RL Algorithm}%
	\put(780,332){Action $a_t$}%
	\put(120,332){State $s_t$}%
	\put(503,122){\small Reward $r_t$}%
	\put(417,60){Environment}%
 \end{overpic}}
 \caption[Reinforcement learning sequence.]{Reinforcement learning sequence: The agent observes the state of the environment~$s_t$ at timestep~$t$. Using its policy~$\pi$, the agent takes an action~$a_t$. This action determines the next state of the environment~$s_{t+1}$, and the reward~$r_{t+1}$.}
 \label{fig:rl_overview}
\end{figure}
}

\noindent
Figure~\ref{fig:rl_overview} illustrates the fundamental steps of the RL approach. RL problems are typically modeled as finite Markov Decision Processes (MDPs), where the agent interacts with the environment over a series of time steps. At each time step~$t$, the agent observes the current state $s_t \in \mathcal{S}$ of the environment. Based on this observation, the agent employs a stochastic policy~$\pi(a_t|s_t)$ to choose an action~$a_t \in \mathcal{A}$. The agent's action results in a transition to a new state~$s_{t+1}\in \mathcal{S}$ at the next time step and the receipt of a numerical reward~$r_{t+1}\in\mathcal{R} \subset \mathbb{R}$. The RL algorithm uses these transitions and rewards to refine the policy, aiming to maximize the expected sum of discounted rewards.

Using the dynamics of the MDP, we can calculate various quantities of interest. The expected reward for a given state-action pair~$(s_t,a_t)$ is represented as a function~$r: \mathcal{S}\times\mathcal{A}\rightarrow \mathbb{R}$. Furthermore, the state-value function~$V_{\pi}(s_t)$ quantifies the expected total discounted reward starting from state~$s_t$ and following policy~$\pi$. Analogously, the action-value function~$Q_{\pi}(s_t, a_t)$ calculates the expected total discounted reward for beginning in state~$s_t$, taking action~$a_t$ and then following the policy~$\pi$. In this context, we refer to the action-value function as value function or Q-function.

\subsection{Soft Actor-Critic Approach}
\label{sec:SAC}
We opt for the SAC algorithm, an actor-critic method, to address the computational challenges of handling continuous state and action spaces in RL. In actor-critic methods, the actor models the policy~$\pi(a_t|s_t)$, while the critic models the value function~$Q_\pi(s_t,a_t)$. SAC is a model-free RL algorithm that maximizes the maximum entropy objective, which aims to increase the policy's entropy while keeping the expected return constant:
\begin{equation}
	\pi^*=\argmax_{\pi}\sum_t \mathbb{E}_{(s_t,a_t)\sim\rho_{\pi}} \left [r(s_t,a_t)+\alpha \mathcal{H}(\pi(\cdot | s_t) \right ],
\end{equation}
with state-action marginal induced by the policy~$\rho_{\pi}(s_t,a_t)$, temperature parameter~$\alpha$, and entropy~$ \mathcal{H}$.
The first part of this objective is equivalent to the classic maximizing total expected reward approach of RL algorithms, and the second term adds the maximum entropy of the policy. The entropy term introduces stochasticity to the policy and thus favors exploration. The temperature parameter controls the trade-off between these two.

SAC is based on Soft Policy Iteration, which alternates between the policy evaluation step, which calculates the soft Q-function of the current policy~$\pi$, and the policy iteration step, which improves the policy in terms of its soft Q-value. For the tabular setting, the convergence of the Soft Policy Iteration towards the optimal policy~$\pi^* \in \Pi$ in terms of the maximum entropy RL is guaranteed.

However, for continuous space and action dimensions, functional approximators are needed for the Q-value function~$Q_{\theta}(s_t,a_t)$ and the policy~$\pi_{\phi}(a_t|s_t)$ with parameters~$\theta$ and $\phi$. For these, iterating the Soft Policy Iteration algorithm until convergence is computationally intractable. Hence, Haarnoja et al. introduce SAC, alternating between optimizing these networks with stochastic gradient descent. The parameters of the soft Q-value functions are minimized using the soft Bellman residual using a target network with parameters~$\bar{\theta}$ to stabilize training:
\begin{equation}
	J_Q(\theta)=\mathbb{E}_{(s_t,a_t)\sim\mathcal{D}}\left[\frac{1}{2}(Q_\theta(s_t,a_t)-(r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim p}\left[V_{\bar{\theta}}(s_{t+1}) \right ]))^2 \right],
\end{equation}
using replay buffer data~$\mathcal{D}$ and discount factor~$\gamma$. The update rule given by stochastic gradient descent is denoted as~$\hat{\triangledown}_{\theta}J_Q(\theta)$.

The policy update minimizes the expected Kullback-Leibler divergence:
\begin{equation}
	J_{\pi}(\phi)=\mathbb{E}_{s_t\sim\mathcal{D},\epsilon_t\sim\mathcal{N}}\left [\alpha \log \pi_{\phi}(f_{\phi}(\epsilon_t;s_t)|s_t)-Q_{\theta}(s_t, f_{\phi}(\epsilon_t;s_t)) \right ],
\end{equation}
where a neural network~$a_t=f_{\phi}(\epsilon_t;s_t)$ reparametrizes the policy with input noise~$\epsilon_t$ to apply the reparametrization trick. This trick enables backpropagation through a sampling operation by transforming the sampling process into a differentiable operation \cite{kingma2013auto}. The update rule is denoted as~$\hat{\triangledown}_{\phi}J_{\pi}(\phi)$.

The temperature parameter~$\alpha$ is essential for the success of SAC and needs to be tuned for each task individually. Even during training of one single task, it is advantageous if~$\alpha$ is adjustable depending on the certainty of the policy about the best action. Thus, Haarnoja et al. present a solution for adjusting the temperature parameter automatically, incorporating an additional objective for learning the temperature value $J(\alpha)$.

For the final SAC algorithm, as given in Algorithm~\ref{alg:sac-update}, Haarnoja et al. propose to use two soft Q-function approximators trained independently to lower the positive bias introduced in the policy improvement step. The \mbox{Q-function} approximators and the policy use the minimum value of these in their updates. 

\begin{algorithm}
\caption{Soft Actor-Critic \cite{haarnoja2018soft}}
\label{alg:sac-update}
\small\textbf{Input:} $\theta_1,\theta_2,\phi$ \Comment{\footnotesize Initial parameters}
\begin{algorithmic}[1]
\small \State $\bar{\theta}_1 \gets \theta_1, \bar{\theta}_2 \gets \theta_2$ \Comment{\footnotesize Initialize target network weights}
\small \State $\mathcal{D} \gets \emptyset$ \Comment{\footnotesize Initialize an empty replay buffer}
\small \For{each iteration}
	\small \For{each environment step}
		\small \State $\mathbf{a}_t \sim \pi_{\phi}(\mathbf{a}_t|\mathbf{s}_t)$ \Comment{\footnotesize Sample action from the policy}
		\small \State $\mathbf{s}_{t+1} \sim p(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)$ \Comment{\footnotesize Sample next state from the environment}
		\small \State $\mathcal{D} \gets \mathcal{D} \cup \{(\mathbf{s}_t,\mathbf{a}_t,r(\mathbf{s}_t,\mathbf{a}_t),\mathbf{s}_{t+1})\}$ \Comment{\footnotesize Store the transition in the replay buffer}
	\small \EndFor
	\small \For{each gradient step}
		\small \State $\theta_i \gets \theta_i - \lambda_Q \hat{\triangledown}_{\theta_i}J_Q(\theta_i)$ for $i \in \{1,2\}$ \Comment{\footnotesize Update Q-function parameters}
		\small \State $\phi \gets \phi - \lambda _{\pi} \hat{\triangledown}_{\phi}J_{\pi}(\phi)$ \Comment{\footnotesize Update policy weights}
		\small \State $\alpha \gets \alpha - \lambda \hat{\triangledown}_{\alpha}J(\alpha)$ \Comment{\footnotesize Adjust temperature}
		\small \State $\bar{\theta}_i \gets \tau\theta_i + (1-\tau)\bar{\theta}_i$ for $i \in \{1,2\}$ \Comment{\footnotesize Update target network weights}
	\small \EndFor
\small \EndFor
\end{algorithmic}
\small \textbf{Output:} $\theta_1,\theta_2,\phi$ \Comment{\footnotesize Optimized parameters}
\end{algorithm}

\section{Social Learning Strategies}
\label{sec:background-social-learning}
\textit{Social learning} is a term initially derived from psychology and has various definitions. Essentially, it refers to the act of learning from others \cite{reed2010social, toyokawa2019social, witt2023social}. In this thesis, we interpret social learning as observing the actions and possibly rewards of demonstrators and incorporating them into the training process of the RL agent. These demonstrators perform actions within the same environment as the RL agent and provide a reference for successful behaviors and strategies. We refer to the RL agent currently trained as a learner. In the following, we distinguish between algorithm-independent methods, which we present first, and specific social algorithms.

\subsection{Algorithm-independent Methods}
This chapter describes two algorithm-independent techniques we integrate into the SAC algorithm. The first technique, called imitation learning, aims to replicate the actions of an experienced demonstrator by assuming the demonstrator's decisions are optimal. The second technique, DB, is a psychological theory used to model the behavior of study participants in RL. According to this theory, the demonstrator's actions are valuable; therefore, the probability that the learner also performs these actions should increase.

\subsubsection*{Imitation Learning}
\textit{Imitation learning} is a technique used to improve the learning of RL agents. The agent is presented with either demonstrator actions or the demonstrator agent itself as input, and it tries to mimic the actions of the demonstrator. This approach assumes that the policy of the demonstrator is optimal. There are various types of imitation learning, such as behavior cloning or inverse RL. We use behavior cloning, where a dataset~$\mathcal{D}=\{(s_t, a_t, r_t, s_{t+1}, d_t)\}$ of demonstration transitions is provided. This dataset enables the direct calculation of the deterministic demonstrator policy~$\pi^*(s_t)$. The policy of the learner~$\pi_{\phi}$ is then trained using the objective
\begin{equation}
	\phi^* = \argmin_{\phi}\mathbb{E}_{(s^*,a^*)\sim \mathcal{D}}\left [\mathcal{L}(a^*, \pi_{\phi}(s^*)) \right ].
\end{equation}
Behavior cloning assumes the data is independent and identically distributed (i.i.d.). Also, there may be a mismatch between the state spaces of the demonstrator and the learner. The training dataset can be created by combining the demonstrator transitions with transitions created by rolling out the learner policy on the environment to overcome these challenges \cite{osa2018algorithmic, ross2011reduction}. 

To integrate the demonstration transitions into the learning algorithm, we store them in a prioritized replay buffer (PRB) before starting training \cite{vecerik2017leveraging, hester2018deep}. This buffer prioritizes the transitions based on their temporal difference (TD) error. Transitions with a higher TD error are prioritized as it assumes they are more challenging to learn and should be observed more frequently during training. The transitions with a higher priority are sampled more likely, and during the training process, the priorities are updated \cite{schaul2015prioritized}.

\subsubsection*{Decision Biasing}
\label{sec:decision-biasing}
DB, also known as frequency-dependent copying, is a method for modeling social learning. In this method, the learner observes the demonstrator's actions and biases its decision to act toward them. The more frequently an action is observed, the more likely it is to be imitated by the learner. Hence, DB assumes the demonstrator's action is valuable. This method is commonly used to study participant behavior, but the experiments we discovered always have discrete action spaces \cite{najar2020actions, witt2023social, toyokawa2019social}. Therefore, we adapt the method for our continuous setting, which we present in Section~\ref{sec:social-agents}.

Our approaches are based on the methods presented by Najar et al., specifically the DB1 and DB2 approaches \cite{najar2020actions}. In their experiment, the available actions are limited to binary choices. The first method adjusts the policy of the learner, denoted by~$\pi$, as follows:
\begin{align*}
 \pi(d) &\leftarrow \pi(d) + \alpha_i\cdot [1- \pi(d)] \\
 \pi(\bar{d}) &\leftarrow 1- \pi(d),
 \numberthis
\end{align*}
where~$d$ is the action of the demonstrator and $\bar{d}$ is the other possible action. The parameter~$\alpha_i$ controls the imitation learning rate (ILR). As a result, the probability of the learner choosing the observed action increases.

The second method uses a decision value function~$Q'$, a copy of the current value function used to derive the policy~$\pi$. $Q'$ is biased towards the observed actions of the demonstrator as follows: 
\begin{equation} 
Q'(d) \leftarrow Q'(d) + \alpha_i \cdot [1-Q'(d)].
\end{equation}

\subsection{MARLISA Algorithm}
\label{sec:marlisa}
This section presents the MARLISA algorithm, a specific algorithm designed for social learning \cite{vazquez2020marlisa}. The MARLISA algorithm is an extension of the SAC method that enables coordination between agents through a collective reward function and the exchange of building-specific information. 

The reward function~$r_{MARL}^b$ focuses on individual and collective goals. It encourages agents to reduce both their own electricity consumption~$e_{b}$ and the total consumption of the entire neighborhood, i.e., all trained buildings~$b \in B$ together:
\begin{equation}
	r_{MARL}^b = -sign(e_b)\cdot0.01(e_b)^2\cdot \min \left \{0, \sum_{b \in B} e_b \right \}.
\end{equation}
The reward is negative when the building~$b$ consumes more energy than it generates, and the neighborhood consumes more energy than it produces overall. The reward is positive if the building produces excess energy, but the neighborhood consumes too much energy. In all other cases, the reward is zero. Although the paper does not describe the scaling factor 0.01, it is included in the implementation. According to Vazquez et al., the exponents are the most effective in empirical research.

The collective reward in MARLISA leads to additional stochasticity, mitigated by exchanging information between agents. By sharing data, the agents can make more accurate predictions about potential rewards for current states and subsequent actions. A gradient-boosting decision tree is trained to predict the future energy consumption of a building, taking into account both the state and the chosen action. The agents are ordered randomly, and each agent selects an action, then estimates the next energy consumption and passes this prediction on to the following agent. This iterative communication allows for improved coordination as the agents gradually refine their actions based on the predictions of the other agents. The algorithm is scalable since each agent only shares two variables with another.

The trained agents are evaluated based on their ability to minimize annual net peak demand, average daily peak demand, total ramp and annual net demand, and maximize the district's average daily load factor. A Rule-Based Controller (RBC) is trained as a baseline, and the cost functions are compared with it. The MARLISA algorithm shows an improvement for all metrics compared to the RBC in different climate zones in the experiments of Vazquez et al. \cite{vazquez2020marlisa}.

