%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Background
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background}
\label{Background}
\todo[inline]{Begriffserkl√§rung: renewable energy (wind, solar, bla)}
This chapter provides the necessary background knowledge to comprehend our approach to the presented problem. First, we introduce the fundamental concepts of reinforcement learning (RL), often modeled as finite Markov decision processes (MDP). However, we also discuss the limitations of this traditional framework in handling continuous state and action spaces. To tackle this challenge, we consider the soft actor-critic (SAC) method a promising solution.
\todo[inline]{TODO}

\section{Reinforcement Learning}
\label{sec:reinforcement-learning}
As defined by Sutton and Barto \cite{sutton2018reinforcement}, RL is a machine learning method that aims to choose actions to maximize a numerical reward. RL agents learn these actions through a trial-and-error approach based on the received rewards. Actions can affect both immediate rewards and long-term rewards. In contrast to supervised methods, RL does not need labeled data for learning and does not try to discover inherent structure in data as unsupervised methods.

\begin{figure}[htb]
     \center
     \begin{overpic}[width=\textwidth, trim={0 1.7cm 0 0.3cm},clip]{figures/rl_overview_blank.pdf}%
	\put(455,415){Agent}%
	\put(450,351){Policy $\pi$}%
	\put(503,290){\small Policy update}%
	\put(409,223){RL Algorithm}%
	\put(780,332){Action $a_t$}%
	\put(120,332){State $s_t$}%
	\put(503,122){\small Reward $r_t$}%
	\put(417,60){Environment}%
    \end{overpic}
  \caption[Reinforcement Learning Overview]{In RL, the agent observes the state of the environment $s_t$ at timestep $t$. The agent takes an action $a_t$ based on the policy $\pi$. This action determines the next state of the environment $s_{t+1}$, and the environment sends a reward $r_{t+1}$ to the agent..}
  \label{fig:rl_overview}
\end{figure}

\noindent
Figure \ref{fig:rl_overview} shows the general RL approach. RL problems are often formulated as finite MDPs, where the agent interacts with the environment over a series of time steps. At each time step $t$, the agent observes the current state of the environment, denoted as $s_t \in \mathcal{S}$. The agent selects an action $a_t \in \mathcal{A}$ using its, in this case stochastic, policy $\pi(a_t|s_t)$, which maps from the state to the probability of selecting the action. As a consequence of the chosen action, the agent receives at the next time step a numerical reward $r_{t+1}\in\mathcal{R} \subset \mathbb{R}$ and a new state $s_{t+1}\in \mathcal{S}$. Both are used in the RL algorithm to optimize the policy to maximize the expected sum of discounted rewards.

The dynamics of the MDP are captured by a function $p': \mathcal{S}\times\mathcal{R}\times\mathcal{S}\times\mathcal{A} \rightarrow [0,1]$. This function represents the probability of transitioning from state $s_t\in\mathcal{S}$ to state $s_{t+1}\in\mathcal{S}$ and receiving reward $r_{t+1}\in\mathcal{R}$ when the action $a_t\in\mathcal{A}$ is taken.
%\begin{equation}
%p(s', r \mid s,a)=P(s_t=s', r_t=r \mid s_{t-1}=s, a_{t-1}=a)
%\end{equation}
Using this function, we can calculate additional quantities of interest. The state transition probability defined as function $p: \mathcal{S}\times\mathcal{S}\times\mathcal{A} \rightarrow [0,1]$ is the probability of transitioning to state $s_{t+1}$ when action $a_t$ is taken, given the current state is $s_t$.
%\begin{equation}
%p(s'\mid s,a)=P(s_t=s'\mid s_{t-1}=s, a_{t-1}=a) = \sum_{r\in\mathcal{R}}p(s',r \mid s,a)
%\end{equation}
Furthermore, we can calculate the expected reward for a given state-action pair $(s_t,a_t)$ as a function $r: \mathcal{S}\times\mathcal{A}\rightarrow \mathbb{R}$.
%\begin{equation}
%r(s_t,a_t)=\mathbb{E}[r_t\mid s_{t-1}=s,a_{t-1}=a]=\sum_{r\in\mathcal{R}}r\sum_{s'\in\mathcal{S}}p(s',r\mid s,a)
%\end{equation}

The state-value function, denoted by $V_{\pi}(s_t)$, represents the expected total discounted reward obtained when starting from state $s_t$ and following policy $\pi$. Similarly, the action-value function, denoted by $Q_{\pi}(s_t, a_t)$ is defined as the expected total discounted reward when starting in state $s_t$, taking action $a_t$ and then following the policy $\pi$. In this context, we refer to the action-value function as value function or Q-function.

\subsection{Soft Actor-Critic}
\label{sec:SAC}
Handling continuous state and action spaces with tabular RL algorithms can be computationally intensive and requires a significant amount of memory. We opted for the SAC algorithm to address this issue. This algorithm is widely used in RL and is designed to efficiently handle continuous state and action spaces. In actor-critic methods, the 'actor' models the policy $\pi(a_t|s_t)$, while the 'critic' models the value function $Q_\pi(s_t,a_t)$. SAC is a model-free RL algorithm that maximizes the maximum entropy objective, which aims to increase the policy's entropy while keeping the expected return constant:
\begin{equation}
	\pi^*=\argmax_{\pi}\sum_t \mathbb{E}_{(s_t,a_t)\sim\rho_{\pi}} \left [r(s_t,a_t)+\alpha \mathcal{H}(\pi(\cdot | s_t) \right ],
\end{equation}
with state-action marginal induced by the policy $\rho_{\pi}(s_t,a_t)$, temperature parameter $\alpha$ and entropy $ \mathcal{H}$.
The first part of this objective is equivalent to the classic maximizing total expected reward approach of RL algorithms, and the second term adds the maximum entropy of the policy. The temperature parameter controls the trade-off between these two. As $\alpha \rightarrow 0$, this objective is the same as in standard RL. The entropy term introduces stochasticity to the policy and thus favors exploration. 

SAC can be derived from the Soft Policy Iteration, an algorithm that alternates between the policy evaluation step, where the soft Q-function of the current policy $\pi$ is calculated, and the policy iteration step, where the policy is improved in terms of its soft Q-value. 
The set of possible policies $\Pi$ is constrained using the Kullback-Leibler (KL) divergence to ensure the tractability of the obtained policy.
For the tabular setting, the convergence of the Soft Policy Iteration towards the optimal policy $\pi^* \in \Pi$ in terms of the maximum entropy RL is guaranteed.

However, for continuous space and action dimensions, functional approximators are needed for the Q-value function $Q_{\theta}(s_t,a_t)$ and the policy $\pi_{\phi}(a_t|s_t)$ with parameters $\theta$ and $\phi$. For these, iterating the Soft Policy Iteration algorithm until convergence is computationally intractable. Hence, the authors introduced SAC, alternating between optimizing these networks with stochastic gradient descent. The parameters of the soft Q-value functions are minimized using the soft Bellman residual using a target network with parameters $\bar{\theta}$ to stabilize training:
\begin{equation}
	J_Q(\theta)=\mathbb{E}_{(s_t,a_t)\sim\mathcal{D}}\left[\frac{1}{2}(Q_\theta(s_t,a_t)-(r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim p}\left[V_{\bar{\theta}}(s_{t+1}) \right ]))^2 \right],
\end{equation}
using replay buffer data $\mathcal{D}$ and discount factor $\gamma$. The update rule given by stochastic gradient descent is denoted as $\hat{\triangledown}_{\theta}J_Q(\theta)$.
%The update rule given by stochastic gradient decent is given by 
%\begin{equation} \small
%	\hat{\triangledown}_{\theta}J_Q(\theta)=\triangledown_{\theta} Q_{\theta}(a_t,s_t)(Q_{\theta}(s_t,a_t)-(r(s_t,a_t)+\gamma(Q_{\bar{\theta}}(s_{t+1},a_{t+1})-\alpha \log (\pi_\phi(a_{t+1}|s_{t+1})))).
%\end{equation}

The objective function for updating the policy is obtained by minimizing the expected KL divergence: 
\begin{equation}
	J_{\pi}(\phi)=\mathbb{E}_{s_t\sim\mathcal{D},\epsilon_t\sim\mathcal{N}}\left [\alpha \log \pi_{\phi}(f_{\phi}(\epsilon_t;s_t)|s_t)-Q_{\theta}(s_t, f_{\phi}(\epsilon_t;s_t)) \right ],
\end{equation}
where the policy is reparametrizied by a neural network $a_t=f_{\phi}(\epsilon_t;s_t)$ with input noise $\epsilon_t$ to apply the reparametrization trick. This trick enables backpropagation through a sampling operation by transforming the sampling process  into a differentiable operation \cite{kingma2013auto}. The update rule is denoted as $\hat{\triangledown}_{\phi}J_{\pi}(\phi)$.
%\begin{equation}
%	\hat{\triangledown}_{\phi}J_{\pi}(\phi)=\triangledown_{\phi}\alpha\log (\pi_{\phi}(a_t|s_t))+(\triangledown_{a_t}\alpha \log (\pi_{\phi}(a_t|s_t))-\triangledown_{a_t}Q(s_t,a_t))\triangledown_{\phi}f_{\phi}(\epsilon_t;s_t).
%\end{equation}

The temperature parameter $\alpha$ is essential for the success of SAC and needs to be tuned for each task individually. Even during training of one single task, it would be advantageous if $\alpha$ would be adjustable: In regions where the policy is very sure about the best action, the entropy should be rather small and thus the policy deterministic. In regions where the policy is unsure, exploration should be enhanced. Thus, the authors presented a solution for adjusting the temperature parameter automatically and proved its theoretical optimality in the tabular setting. For this, the new objective considers the target minimum entropy $\mathcal{H}$ in a constraint to the classical maximum reward approach:
\begin{equation}
	\max_{\pi_{0:T}}\mathbb{E}_{\rho_{\pi}}\left[\sum_{t=0}^Tr(s_t,a_t) \right ] \textbf{ s.t. } \mathbb{E}_{(s_t,a_t)\sim \rho_{\pi}}\left [-\log(\pi_t(a_t|s_t)) \right ] \geq \mathcal{H}~\forall t
\end{equation}
After solving for the optimal soft Q-value function $Q^*_t$ and the optimal policy $\pi^*_t$, the optimal temperature parameter at time step $t$ is given by
\begin{equation}\label{eq:obj-alpha}
	\alpha^*_t = \argmin_{\alpha_t} \mathbb{E}_{a_t \sim \pi^*_t}\left [-\alpha_t \log \pi^*_t(a_t|s_t; \alpha_t)-\alpha_t \bar{\mathcal{H}} \right ].
\end{equation}
For the final SAC algorithm, as given in Algorithm \ref{alg:sac-update}, the authors propose to use two soft Q-function approximators trained independently to lower the positive bias introduced in the policy improvement step. The minimum value of these is used for updating both Q-function approximators as well as the policy. The temperature parameter $\alpha$ is learned by computing the gradient of the objective given in Equation \ref{eq:obj-alpha}, where $Q_t$ and $\pi_t$ are assumed to be optimal for the current time step.


\begin{algorithm}
\caption{Soft Actor-Critic \cite{haarnoja2018soft}}
\label{alg:sac-update}
\small\textbf{Input:}   $\theta_1,\theta_2,\phi$ \Comment{\footnotesize Initial parameters}
\begin{algorithmic}[1]
\small \State $\bar{\theta}_1 \gets \theta_1, \bar{\theta}_2 \gets \theta_2$ \Comment{\footnotesize Initialize target network weights}
\small \State $\mathcal{D} \gets \emptyset$ \Comment{\footnotesize Initialize an empty replay buffer}
\small \For{each iteration}
	\small \For{each environment step}
		\small \State $\mathbf{a}_t \sim \pi_{\phi}(\mathbf{a}_t|\mathbf{s}_t)$ \Comment{\footnotesize Sample action from the policy}
		\small \State $\mathbf{s}_{t+1} \sim p(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)$ \Comment{\footnotesize Sample next state from the environment}
		\small \State $\mathcal{D} \gets \mathcal{D} \cup \{(\mathbf{s}_t,\mathbf{a}_t,r(\mathbf{s}_t,\mathbf{a}_t),\mathbf{s}_{t+1})\}$ \Comment{\footnotesize Store the transition in the replay buffer}
	\small \EndFor
	\small \For{each gradient step}
		\small \State $\theta_i \gets \theta_i - \lambda_Q \hat{\triangledown}_{\theta_i}J_Q(\theta_i)$ for $i \in \{1,2\}$ \Comment{\footnotesize Update Q-function parameters}
		\small \State $\phi \gets \phi - \lambda _{\pi} \hat{\triangledown}_{\phi}J_{\pi}(\phi)$ \Comment{\footnotesize Update policy weights}
		\small \State $\alpha \gets \alpha - \lambda  \hat{\triangledown}_{\alpha}J(\alpha)$ \Comment{\footnotesize Adjust temperature}
		\small \State $\bar{\theta}_i \gets \tau\theta_i + (1-\tau)\bar{\theta}_i$ for $i \in \{1,2\}$ \Comment{\footnotesize Update target network weights}
	\small \EndFor
\small \EndFor
\end{algorithmic}
\small \textbf{Output:}   $\theta_1,\theta_2,\phi$ \Comment{\footnotesize Optimized parameters}
\end{algorithm}

\subsection{Deep Deterministic Policy Gradient}
\cite{lillicrap2015continuous}

\section{Social Learning}
\label{sec:background-social-learning}


\cite{hester2018deep}


\textit{Social learning} is a term initially from psychology and has various definitions. Essentially, it refers to the act of learning from others \cite{reed2010social, toyokawa2019social, witt2023social}. In this thesis, we interpret social learning as observing the actions and possibly rewards of demonstrators and incorporating them into the training process of the RL agent.

\subsection{Imitation Learning}
Demonstrator transitions in replay buffer \cite{vecerik2017leveraging}
Imitation learning (soll ja nicht unbedingt imitieren, sondern 'miteinander arbeiten', kein human demonstrator)

\subsection{Decision Biasing}
\label{sec:decision-biasing}
Decision Biasing is a method for modeling social learning. It observes which actions the demonstrator takes and assumes that they are good. Therefore, the policy to be trained is biased towards the demonstrator's actions. In the literature, this method is used to model the behavior of study participants. However, the experiments always have discrete, often even only binary, action spaces. Therefore, we had to adapt the method for our continuous setting, which we present in section \ref{sec:met-mat}. 

Our approach is based on the 'DB2' method presented by Najar et al. \cite{najar2020actions}. In their experiment, the action space is binary. A 'decision value function' $Q'$ is used, which is a copy of the current value function used to derive the policy $\pi$. $Q'$ is biased towards the observed demonstrator actions as follows: 
\begin{equation} 
Q'(d) \leftarrow Q'(d) + \alpha_i \cdot [1-Q'(d)],
\end{equation}
where $d$ is the observed demonstrator action and $\alpha_i $ is an imitation decision bias rate parameter. 