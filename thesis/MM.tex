%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Grundlagen
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Datasets}
\todo[inline]{(all units in kWh) ????!!!! Also in the following!!!!!!!!!!}
\subsection{Building data}
\label{sec:building-data}
As building data, we used the one provided by CityLearn for the CityLearn Challenge 2021 \cite{citylearn-challenge}. These datasets contain the battery and PV specifications for 13 buildings. Each of them has a battery with attributes as specified in Table \ref{tab:battery-attributes} and a PV with a nominal power of 5 \textcolor{red}{kW} for building 4 and buildings 10-17. All other buildings have a PV with nominal power 4.

Moreover, for each building, the equioment electric power and the solar generation time series per hour is provided. The input variables are described in Appendix \ref{tab:buildings-vars}. Since we used different weather data than the one in the challenge, we also resimulated the solar generation time series of the buildings. For this, we used the python library pvlib \textcolor{red}{TODO source} and followed there simulation instructions on their website\footnote{See \url{https://pvlib-python.readthedocs.io/en/latest/gallery/adr-pvarray/plot_simulate_fast.html}}. For the weather data, we used the one described in Chapter \ref{sec:weather-data}. As location, we used \textcolor{red}{TODO}. Finally, for \textcolor{red}{TODO P\_STC} we used the maximum solar generation of the original data and added 500 W in order to meet approximately the same mean generation of all buildings as in the original dataset.

To obtain the median values for the energy consumption per hour with storage and PV, without storage, and without storage and PV, we trained the \textcolor{red}{SAC} agent with hyperparameter values \textcolor{red}{bliblablu} \textcolor{red}{n} episodes on \textcolor{red}{the data} and calculated the median of these values. The obtained medians are given in Appendix \ref{tab:building-medians}.

\begin{table}[htb]
\begin{tabularx}{\linewidth}{lXc}
Attribute name & Description & Value \\ \hline
Capacity & Maximum amount of energy the storage device can store & 6.4 kWh \\
Efficiency & Technical efficiency & 90 \% \\
Capacity loss coefficient & Storage capacity lost in each charge and discharge cycle (as a fraction of the total capacity) & $1e^{-5}$ \\
Loss coefficient & Standby hourly losses & 0 \\
Nominal power & Maximum amount of electric power that the battery can use to charge or discharge & 5 \textcolor{red}{kW}
\end{tabularx}
\caption[Attributes of the batteries of each building]{Attributes of the batteries of each building. Description of the attributes obtained from \textcolor{red}{TODO source}}
\label{tab:battery-attributes}
\end{table}

\subsection{Fuelmix and Weather data}
\label{sec:weather-data}
The fuel mix data we used is obtained from the \textcolor{red}{NY-ISO}, retrieved on the 7th of June 2023 \cite{fuelmix_nyiso}. The original data provides the produced energy in MW for each of the energy sources dual fuel, natural gas, nuclear, \textcolor{red}{other fossil fuels}, wind, hydro, and \textcolor{red}{other renewables} for a time interval of 5 minutes for 2021 in the New York (NY) state. Since we are only interested in renewable (wind, hydro, other renewables) or non-renewable (all others) energy produced, we summed these values up and calculated the value in kWh. \todo[inline]{kW vs MW vs kWh????} To meed the hourly time steps of the building data, we calculated the hourly mean of the values, because \textcolor{red}{???}. The final fuelmix data contains the hourly amount of energy produced with renewable sources in kWh and the share of renewable energy produced calculated as the absolute amount of renewable energy produced devided by the total produced energy. The renewable share is calculated on the original time steps of five minutes, so for the final dataset here also the median per hour was calculated. Note, that this data does not include the solar generation of the buildings described in Chapter \ref{sec:building-data}, but only the renewable energy from the grid. 

As weather data, we used data from the New York state too, in order to encourage correlation between the weather and the produced amount of renewable energy. We obtained the data from the \textcolor{red}{NSRDB} \cite{weather_nsrdb}. 
\todo[inline]{foecasts}

\todo[inline]{fuel mix scaled to 0.5 (somtetimes buildings use full available from grid -> capture this behaviour)}

\subsection{Energy prices}
For the energy prices, we used the pricing data set provided with the CityLearn challenge 2022 \cite{citylearn-challenge} and weighted the price information by the share of fossil energy in the grid. The basic dataset follows a electricity rate, where the electricity is cheaper in the early morning, late at night and during the months october to may. The detailed electricity prices are shown in Table \ref{tab:basic-prices}.
\begin{table}[htb]
\center
\begin{tabular}{l|l|l|l|l}
            & \multicolumn{2}{l|}{June - September} & \multicolumn{2}{l}{Oktober - May} \\
Time        & Weekday           & Weekend          & Weekday         & Weekend         \\ \hline
8 AM - 4 PM & 0.21                  &  0.21                & 0.20                 & 0.20                 \\
4 PM - 9 PM & 0.54                  &  0.40                 & 0.50                & 0.50                \\
9 PM - 8 AM & 0.21                  & 0.21                 &  0.20               &  0.20              
\end{tabular}
\caption[Electricity price rate CityLearn Challenge 2022]{Electricity price rate of the electricity price data set provided with the CityLearn Challenge 2022.}
\label{tab:basic-prices}
\end{table}

We weighted these base prices $p_{base,t}$ at time step $t$ by the share of fossil energy using a weighting factor $\beta$:
\begin{equation}
	p_t = p_{base,t} + \beta  \cdot (1-\frac{E_{r,grid}}{E_{grid}}),
\end{equation}
where  $E_{r,grid}$ is the renewable amount of energy produced in the grid and $E_{grid}$ is the total amount of energy produced in the grid. Finally, we normalized the prices to be in the range between zero and one.

\todo[inline]{foecasts}

\section{Key Performance Indicators}
We used three KPIs for evaluating the performance of the models. We first present the cost functions used for calculating the KPIs and then explain the calculation and interpretation of the KPIs.

First of all, we calculate the positive net electricity consumption of all buildings $E_{net_{pos}}$ as the sum of the non-negative net electricity consumptions of all buildings $e_{net}^b$:
\begin{equation}
  E_{net_{pos}} = \sum_{b\in Buildings} \max(e_{net}^b, 0)
\end{equation}
\todo[inline]{solar energy refers to the one produced by the buildings, not grid}
The consumption of an individual building is calculated as described in Section \ref{sec:city-learn-presentation} and thus can also reach negative values if, for example, more solar energy is produced than consumed. Since buildings cannot share energy produced of the PVs, we use the positive consumption to prevent this. Next, we calculate the amount of used solar energy per building as the minimum of the net energy consumption without the solar energy $e_{pv}^b$ and the negative solar energy since these values are negative numbers. To get the total solar consumption of the neighborhood $E_{used_{pv}}$, we sum these values up:
\begin{equation}
  E_{used_{pv}} = \sum_{b\in Buildings} \max(\min(e_{net}^b - e_{pv}^b, - e_{pv}^b), 0)
\end{equation}
 Again non-negative values are used, to prevent negative values which can origin due to discharging energy storages more than needed. In the next step, we calculate the used renewable energy from the grid $E_{used_{r,grid}}$ as the minimum of the positive net energy consumption of all buildings, which is eqivalent to the needed energy from the grid, and the available renewable energy in the grid $E_{r,grid}$: 
\begin{equation}
E_{used_{r,grid}} = \min(E_{net_{pos}}, E_{r, grid})
\end{equation}
To calculate the total renewable energy consumption $E_{used_{r}}$, we sum both values up:
\begin{equation}
  E_{used_{r}} = E_{used_{r,grid}}  + E_{used_{pv}}
\end{equation}
To calculate the share of consumed renewable energy from the total consumed energy $R_{share}$, we divide this by the sum of the positive net energy consumption (so the energy consumed from the grid) and the used solar energy:
\begin{equation}
 R_{share} = \frac{E_{used_{r}}}{E_{net_{pos}} + E_{used_{pv}}}
\end{equation}
Using these, we calculate the cost function $1-average\_daily\_renewable\_share$ as the mean consumed renewable share of a day:
\begin{equation}
 1 - average\_daily\_renewable\_share = \sum_{h=1}^{24}\frac{R_{share}^h}{24}
\end{equation}
Notice that we omitted the time index $h$ in the equations before for readability, but all of them equal the calculation for one time step. As the data is provided in hourly timesteps, this equals one hour.

The next cost function $1 - average\_daily\_renewable\_share\_grid$ is very similar, but only observes the share of renewable energy of the energy consumed from the grid $R_{share,grid}$ calculated as
\begin{equation}
 R_{share,grid} = \frac{E_{used_{r,grid}}}{E_{net_{pos}}}.
\end{equation}
The KPI is then calculated as
\begin{equation}
 1 - average\_daily\_renewable\_share\_grid = \sum_{h=1}^{24}\frac{R_{share, grid}^h}{24},
\end{equation}
where again the time index is omitted before.

The last cost function considers only the consumed solar energy of the buildings. For this, we calculate the share of used solar energy of the produced PV energy $E_{pv}^h$ of all buildings:
\begin{equation}
 1 - used\_pv\_of\_total = \sum_{h=1}^{24}\frac{\frac{E_{used_{pv}}^h}{E_{pv}^h}}{24},
\end{equation}
For all of the cost function it holds that low values indicate better performance than higher values.

To calculate the KPIs we calculate the ratio between a cost function for the values when using storage and when not using storage. The values when not using storage are obtained similar as the formulas above, but using the (positive) net electricity consumptions without storage already implemented in the CityLearn framework (see Section \ref{sec:city-learn-presentation}). Also, the values without storage are similar to the energy demand of the buildings that is not covered by the PV production at exact this time step. KPI values lower than 1 indicate better performance compared to using no storage and values higher than 1 indicate worse performance. 

\section{Implemented Agents}
\subsection{SAC Baseline}
\todo[inline]{Übergebene parameter prüfen! auch layergröße zb}
As a baseline we implemented a SAC agent following the calculations as presented in Section \ref{sec:SAC}. We used the implementation provided by CityLearn and extended it to our needs, including the implementation of autotuning the temperature parameter $\alpha$. Moreover, we extended the implementation by the options to clip the gradient of both, critic and policy networks, and to use kaiming initialization and the mean squared error loss for the critic networks.
\todo[inline]{explain kaiming initialization , ReLU somewhere, reparametrization trick}

The critic networks are implemented as 3-layer feedforward networks with 400 neurons in the first, and 300 neurons in the second hidden layer using layer normalization and ReLU activation. If the kaiming initialization is not used, the initial weights and biases are drawn from the uniform distribution $\mathcal{U}(-0.003, 0.003)$. The output is the estimated soft Q-Value.

The policy is implemented as a Gaussian distribution where the mean and the covariance is approximated by neural networks. Those share the same backbone, a 3-layer feed forward network similar to the critic networks, but without layer normalization. The network has two different output layers, one for calculating the mean and one for the log standard deviation. The latter is forced to be in a defined range between -20 and 2. To sample an action, the mean and standard deviation are calculated by a forward pass of the network. Then, the reparametrization trick is used to sample actions from the defined Gaussian and the obtained actions are transformed in order to match the defined action space. Additionally, the log probability of the obtained actions is calculated. 

The observation space of the SAC agent is listed in Appendix \ref{app:observation-space-sac}.

\subsection{SAC using demonstrator transitions}
\todo[inline]{Font in tabellen beschreibungen verkleinern}
As mentioned in Section \ref{sec:background-social-learning}, a widely used method to improve learning is sampling demonstrator transitions and storing them in a Prioritized Replay Buffer. These kind of buffer add priorities based on the Temporal difference (TD) error to the transitions: Transistion with a higher TD error have an higher priority, assuming that these are more difficult to learn and hence should be seen more often during training. Thus, transitions with a higher priority are sampled with a higher probability. The priorities are updated during the training process \cite{schaul2015prioritized}.

\todo[inline]{Present the buildings used in training and the demonstrator building (and why this)}
For implementing the SAC agent using the demonstrator transitions, we first trained one building with 

\subsection{\textcolor{red}{Social Agent I}}
\textcolor{red}{Policy update}
First: normal policy update

Then: policy update using demonstrator actions

Classical loss $l$ to minimize is is 
\begin{align*}
	l &= \mathbb{E}_{s_t \sim \mathcal{D}, \epsilon_t \sim \mathcal{N}}[\alpha \log\pi_{\phi}(f_{\phi}(\epsilon_t;s_t)|s_t)-Q_{\theta}(s_t, f_{\phi}(\epsilon_t;s_t))] \\ 
	&=  \mathbb{E}_{s_t \sim \mathcal{D}, \epsilon_t \sim \mathcal{N}}[\alpha \mathcal{H}_{SAC}-\mathcal{V}_{SAC}] \numberthis \label{eqn:policy-loss}
\end{align*}
with the entropy term that aims to maximize randomness $\mathcal{H}_{SAC}$ and the value term that aims to maximize the estimated Q-Value of the action $\mathcal{V}_{SAC}$. For incorporating demonstrator actions similar to decision biasing described in chapter \ref{sec:decision-biasing}, so increase the probability of observed demonstrator actions or increase the estimated Q-Value of these, we modify the terms as follows: In mode 1, we use the actions sampled from the demonstrator $f^d_{\Phi}(\epsilon_t;s_t)$ in the value term and increase the estimated Q-Value by adding a fraction of the absolute value of it using the imitation learning rate $\alpha_i$:
\begin{equation}
	\mathcal{V}_{M1} =  Q_{\theta}(s_t, f_{\phi}^d(\epsilon_t;s_t)) + \alpha_i|Q_{\theta}(s_t, f_{\phi}^d(\epsilon_t;s_t))|
\end{equation}
Also, the probability of taking the demonstrator action for the demonstrator is used in the entropy term:
\begin{equation}
	\mathcal{H}_{M1} =  \log\pi_{\phi}^d(f_{\phi}^d(\epsilon_t;s_t)|s_t)
\end{equation}
In mode 2, the probability of taking the demonstrator action is increased by the learning rate:
\begin{equation}
	\mathcal{H}_{M2} = \log\pi_{\phi}^d(f_{\phi}^d(\epsilon_t;s_t)|s_t) + \alpha_i|\log\pi_{\phi}^d(f_{\phi}^d(\epsilon_t;s_t)|s_t)|
\end{equation}
Mode 3 combines mode 1 and mode 2. Mode 4-6 are similar to the modes 1-3, but the probability of taking the demonstrator action in the current trained policy is used:
\begin{equation}
	\mathcal{H}_{M4} =  \log\pi_{\phi}(f_{\phi}^d(\epsilon_t;s_t)|s_t)
\end{equation}
\begin{equation}
	\mathcal{H}_{M5} = \log\pi_{\phi}(f_{\phi}^d(\epsilon_t;s_t)|s_t) + \alpha_i|\log\pi_{\phi}(f_{\phi}^d(\epsilon_t;s_t)|s_t)|
\end{equation}
All modes calculate the loss as defined in Equation \ref{eqn:policy-loss} using the defined entropy and value term. Table \ref{tab:social-policy-update-modes} summarizes the different modes and their entropy and value terms.

{
\renewcommand{\baselinestretch}{0.9} 
\normalsize
\begin{table}[htb]
\center
\begin{tabular}{|l|l|l|}
\hline
    \textbf{Mode} & \textbf{Entropy term} & \textbf{Value term} \\
  \hline\hline
  1 & $\mathcal{H}_{M1}$ &$\mathcal{V}_{M1}$
  \\ \hline
  2 & $\mathcal{H}_{M2}$& $\mathcal{V}_{SAC}$ \\ \hline
  3 & $\mathcal{H}_{M2}$& $\mathcal{V}_{M1}$ \\ \hline 
  4 & $\mathcal{H}_{M4}$ & $\mathcal{V}_{M1}$ \\ \hline
  5 & $\mathcal{H}_{M5}$ & $\mathcal{V}_{SAC}$ \\ \hline
  6 & $\mathcal{H}_{M5}$ &  $\mathcal{V}_{M1}$ \\ \hline 
\end{tabular}
  \caption[Modes for Social Agent focusing on the policy update]{Modes for Social Agent focusing on the policy update. Mode 1 increases the estimated Q-Value of the demonstrator action, mode 2 increases the probability of the demonstrator action and mode 3 combines both. Mode 4-6 are similar, but the probability of the demonstrator actions in the current trained policy is used instead of their probabilities in the demonstrator policy.}
  \label{tab:social-policy-update-modes}
\end{table}
}


\subsection{\textcolor{red}{Social Agent II}}
Since the social policy update did not improve the performance of the models, we decided to focus on the Q-Value update in the next step. For this, we also first conducted classical Q-Value and policy updates as defined in Algorithm \ref{alg:sac-update}. Next, we performed a social Q-Value update on both Q-Value networks where we defined the target values as follows:
\begin{equation}
	Q_{\bar{\theta}}(a_t^d,s_t) + \alpha_i|Q_{\bar{\theta}}(a_t^d,s_t)| 
\end{equation}
This follows the assumption of decision biasing, that actions taken by the demonstrator $a_t^d$ are good and should thus have a higher Q-Value as proposed by \cite{najar2020actions}. Afterwards, a soft update of the target networks parameters is performed, and, optionally, a standard policy update.

\section{Training}
\subsection{Hyperparameters}
\todo[inline]{episodes, steps, hyperparameters, ReplayBuffer, for each building one agent, actions

MM oder results?!}
\begin{itemize}
	\item Rewards
	\begin{itemize}
	\item PricePv (alpha 0, 0.25, 0.5, 0.75, 1)
	\item fossilpenalty
	\item tolovski \cite{tolovski2020advancing}
	\end{itemize}
	\item Episodes
	\begin{itemize}
	\item 2, 3, 4
	\end{itemize}
	\item Hyperparameters
	\begin{itemize}
	\item Pricing factor (10,20) \textcolor{red}{lowe?}
	\item Normalize/not normalize price
	\item limit observations
	\item increase batch size (1024)
	\item autotune alpha
	\item gradient clipping
	\item kaiming initialization \cite{he2015delving}
	\item L2 loss
	\item Discount factor: 0.96,0.97,0.98,0.99
	\item longer training (start earlier)
	\item change renewable production in grid
	\end{itemize}
\end{itemize}

\subsection{Reward function}
The reward function we used is a linear combination of two other reward functions, where the parameter $\alpha$ determines the influence of them. The reward $r_t^b$ is calculated for each building $b$, as for each building one agent is trained, at time step $t$:
\begin{equation}
r_t^b = \alpha r_{price_t}^b + (1-\alpha)r_{solar_t}^b.
\end{equation}
The individual reward functions used are the price penalty rewad $r_{price_t}^b$ and the solar penalty reward $r_{solar_t}^b$. The former is calculated as the minimum between the negative price payed for the energy obtained from the grid and zero:
\begin{equation}
r_{price_t}^b = \min(-p_{net_t}^b, 0)
\end{equation}
where $p_{net}^b$ are the costs of the net electricity consumption of building $b$ calculated as the product of the net electricity consumption and the electricity price at time step $t$. The minimum term prevents positive costs that can occur when the net electricity consumption is negative. 

The solar penalty reward is already provided by CityLearn and aims to maximize the used solar energy by penalizing obtaining energy from the grid when the batteries are loaded and penalizing not using solar energy for loading the batteries if they are not fully loaded:
\begin{equation}
r_{solar_t}^{b} = \left\{%
\begin{array}{ll}
    -(1+sign(e_{net_t}^{b}) \cdot \frac{SOC^b_t}{C^b_t}) \cdot |e_{net_t}^{b}|, & \hbox{if } C^b_t > 0.00001  \\
    0, & \hbox{else}
\end{array}%
\right..
\end{equation}
Here, $C^b_t$ is the capacity of the battery of building $b$ at time step $t$ and $SOC^b_t$ is the state of charge of this battery, so the ration calculates the percent of load status. Table \ref{tab:solar-penalty-reward} shows, that the solar penalty reward is zero if the building has a balanced energy consumption or if the net energy consumption is negative and the battery is fully charged. In all other cases the reward is negative. If the battery is fully charged but the building obtains energy from the grid, the penalty is maximized.
{
\renewcommand{\baselinestretch}{0.9} 
\normalsize
\begin{table}[htb]
\center
\begin{tabular}{|c|c||c|}
\hline
   \textbf{$e_{net_t}^b$}  & \textbf{$\frac{SOC^b_t}{C^b_t}$} & \textbf{$r_{solar_t}^b$} \\
  \hline\hline
  $<0$ & $0$ & $<0$ \\
  $<0$ & $>0$ and $<1$ & $<0$ \\
  $<0$ & $1$ &  0\\
  $0$ & $0$ &  0\\
  $0$ & $>0$ and $<1$ &  0\\
  $0$ & $1$ &  0\\
  $>0$ & $0$ & $<0$ \\
  $>0$ & $>0$ and $<1$ & $<0$  \\
  $>0$ & $1$ & $<0$ \\
  \hline
\end{tabular}
  \caption[Beispieltabelle mit einer langen Legende]{Beispieltabelle mit einer langen Legende, damit man sieht, dass in der Legende der Zeilenabstand verringert wurde. Ausserdem soll auch der Font etwas kleiner gew\"ahlt werden. So sieht die ganze Umgebung kompakter aus.}
  \label{tab:solar-penalty-reward}
\end{table}
}

