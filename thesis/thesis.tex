%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% LaTeX-Rahmen fuer das Erstellen von Masterarbeiten
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% allgemeine Einstellungen
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[twoside,12pt,a4paper]{report}
%\usepackage{reportpage}
\usepackage{epsf}
\usepackage{graphics, graphicx}
\usepackage{latexsym}
\usepackage[margin=10pt,font=small,labelfont=bf]{caption}
\usepackage[utf8]{inputenc}
\usepackage[toc,page]{appendix}


% Own packages
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{url}
\usepackage{amsmath}
\usepackage[permil]{overpic}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\textwidth 14cm
\textheight 22cm
\topmargin 0.0cm
\evensidemargin 1cm
\oddsidemargin 1cm
%\footskip 2cm
\parskip0.5explus0.1exminus0.1ex

% Kann von Student auch nach pers\"onlichem Geschmack ver\"andert werden.
\pagestyle{headings}

\sloppy

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% hier steht die neue Titelseite 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
\begin{titlepage}
 \begin{center}
  {\LARGE Eberhard Karls Universit\"at T\"ubingen}\\
  {\large Mathematisch-Naturwissenschaftliche Fakult\"at \\
Wilhelm-Schickard-Institut f\"ur Informatik\\[4cm]}
  {\huge Master Thesis Computer Science\\[2cm]}
  {\Large\bf  Incorporating Social Learning into Multi-Agent Reinforcement Learning
to \\Lower Carbon Emissions in Energy Systems\\[1.5cm]}
 {\large Kristina Lietz}\\[0.5cm]
01.12.2023\\[4cm]
{\small\bf Reviewers}\\[0.5cm]
  \parbox{7cm}{\begin{center}{\large Dr. Nicole Ludwig}\\
   (Bioinformatik)\\
  {\footnotesize Wilhelm-Schickard-Institut f\"ur Informatik\\
	Universit\"at T\"ubingen}\end{center}}\hfill\parbox{7cm}{\begin{center}
  {\large Prof. Setareh Maghsudit}\\
  (Biologie/Medizin)\\
  {\footnotesize Medizinische Fakult\"at\\
	Universit\"at T\"ubingen}\end{center}
 }
  \end{center}
\end{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Titelr"uckseite: Bibliographische Angaben
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thispagestyle{empty}
\vspace*{\fill}
\begin{minipage}{11.2cm}
\textbf{Lietz, Kristina:}\\
\emph{Incorporating Social Learning into Multi-Agent Reinforcement Learning
to Lower Carbon Emissions in Energy Systems}\\ Master Thesis Computer Science\\
Eberhard Karls Universit\"at T\"ubingen\\
Thesis period: 01.06.2023~-~01.12.2023
\end{minipage}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagenumbering{roman}
\setcounter{page}{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Seite I: Zusammenfassug, Danksagung
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section*{Abstract}

\todo[inline]{Write here your abstract.}

\newpage
\section*{Zusammenfassung}

\todo[inline]{Bei einer englischen Masterarbeit muss zus\"atzlich eine deutsche Zusammenfassung verfasst werden.}

\newpage
\section*{Acknowledgements}

\todo[inline]{Write here your acknowledgements.}

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Table of Contents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\baselinestretch}{1.3}
\small\normalsize

\tableofcontents

\renewcommand{\baselinestretch}{1}
\small\normalsize

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% List of Figures
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\baselinestretch}{1.3}
\small\normalsize

\addcontentsline{toc}{chapter}{List of Figures}
\listoffigures

\renewcommand{\baselinestretch}{1}
\small\normalsize

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% List of tables
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\baselinestretch}{1.3}
\small\normalsize

\addcontentsline{toc}{chapter}{List of Tables}
\listoftables

\renewcommand{\baselinestretch}{1}
\small\normalsize

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% List of abbreviations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% can be removed
\addcontentsline{toc}{chapter}{List of Abbreviations}
\chapter*{List of Abbreviations\markboth{LIST OF ABBREVIATIONS}{LIST OF ABBREVIATIONS}}
\todo[inline]{TODO}
\begin{tabbing}
\textbf{FACTOTUM}\hspace{1cm}\=Schrott\kill
\textbf{KL divergence}\>Kullback-Leibler divergence \\
\textbf{KPI}\>Key performance indicator \\
\textbf{kWh}\>Kilowatt hour \\
\textbf{MDP}\>Markov decision process \\
\textbf{NY}\> New York \\
\textbf{PV}\> Photovoltaic \\
\textbf{RL}\>Reinforcement learning \\
\textbf{SAC}\>Soft actor-critic \\
\textbf{SOC}\> State of the charge \\
\textbf{TD error}\> Temporal Difference error \\
\textbf{...} \> ...\\
\end{tabbing}

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Der Haupttext, ab hier mit arabischer Numerierung
%%% Mit \input{dateiname} werden die Datei `dateiname' eingebunden
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagenumbering{arabic}
\setcounter{page}{1}

%% Introduction
\input{intro}
\begin{figure}[htb]
     \centerline{\epsffile{figures/chordal.eps}}
  \caption{Chordale Graphen}
  \label{fig2.1}
\end{figure}

Abbildung~\ref{fig2.1} zeigt ...

{
\renewcommand{\baselinestretch}{0.9} 
\normalsize
\begin{table}[htb]
\begin{tabular}{|p{2.7cm}||l|c|r|}
\hline
    \textbf{Spalte 1} 
  & \textbf{Spalte 2} 
  & \textbf{Spalte 3} 
  & \textbf{Spalte 4} \\
  \hline\hline
  xxx1111
  & xxxxxxx2222222
  & xxxxxx333333 
  & xxxxxxxxxx444444 \\
  \hline
    ...
  & ...
  & ...
  & ...\\
  \hline
\end{tabular}
  \caption[Beispieltabelle mit einer langen Legende]{Beispieltabelle mit einer langen Legende, damit man sieht, dass in der Legende der Zeilenabstand verringert wurde. Ausserdem soll auch der Font etwas kleiner gew\"ahlt werden. So sieht die ganze Umgebung kompakter aus.}
  \label{tabelle-1}
\end{table}
}
Referenzen: \cite{SaaSchTue97,TueConSaa96ismis,SchTueSaa98preprint}

\cleardoublepage

%% 
\chapter{Background}
\label{Background}
\todo[inline]{Begriffserklärung: renewable energy (wind, solar, bla)}
This chapter provides the necessary background knowledge to comprehend our approach to the presented problem. First, we introduce the fundamental concepts of reinforcement learning (RL), often modeled as finite Markov decision processes (MDP). However, we also discuss the limitations of this traditional framework in handling continuous state and action spaces. To tackle this challenge, we consider the soft actor-critic (SAC) method a promising solution.
\todo[inline]{TODO}

\section{Reinforcement Learning}
\label{sec:reinforcement-learning}
As defined by Sutton and Barto \cite{sutton2018reinforcement}, RL is a machine learning method that aims to choose actions to maximize a numerical reward. RL agents learn these actions through a trial-and-error approach based on the received rewards. Actions can affect both immediate rewards and long-term rewards. In contrast to supervised methods, RL does not need labeled data for learning and does not try to discover inherent structure in data as unsupervised methods.

\begin{figure}[htb]
     \center
     \begin{overpic}[width=\textwidth, trim={0 1.7cm 0 0.3cm},clip]{figures/rl_overview_blank.pdf}%
	\put(455,415){Agent}%
	\put(450,351){Policy $\pi$}%
	\put(503,290){\small Policy update}%
	\put(409,223){RL Algorithm}%
	\put(780,332){Action $a_t$}%
	\put(120,332){State $s_t$}%
	\put(503,122){\small Reward $r_t$}%
	\put(417,60){Environment}%
    \end{overpic}
  \caption[Reinforcement Learning Overview]{In RL, the agent observes the state of the environment $s_t$ at timestep $t$. The agent takes an action $a_t$ based on the policy $\pi$. This action determines the next state of the environment $s_{t+1}$, and the environment sends a reward $r_{t+1}$ to the agent..}
  \label{fig:rl_overview}
\end{figure}

\noindent
Figure \ref{fig:rl_overview} shows the general RL approach. RL problems are often formulated as finite MDPs, where the agent interacts with the environment over a series of time steps. At each time step $t$, the agent observes the current state of the environment, denoted as $s_t \in \mathcal{S}$. The agent selects an action $a_t \in \mathcal{A}$ using its, in this case stochastic, policy $\pi(a_t|s_t)$, which maps from the state to the probability of selecting the action. As a consequence of the chosen action, the agent receives at the next time step a numerical reward $r_{t+1}\in\mathcal{R} \subset \mathbb{R}$ and a new state $s_{t+1}\in \mathcal{S}$. Both are used in the RL algorithm to optimize the policy to maximize the expected sum of discounted rewards.

The dynamics of the MDP are captured by a function $p': \mathcal{S}\times\mathcal{R}\times\mathcal{S}\times\mathcal{A} \rightarrow [0,1]$. This function represents the probability of transitioning from state $s_t\in\mathcal{S}$ to state $s_{t+1}\in\mathcal{S}$ and receiving reward $r_{t+1}\in\mathcal{R}$ when the action $a_t\in\mathcal{A}$ is taken.
%\begin{equation}
%p(s', r \mid s,a)=P(s_t=s', r_t=r \mid s_{t-1}=s, a_{t-1}=a)
%\end{equation}
Using this function, we can calculate additional quantities of interest. The state transition probability defined as function $p: \mathcal{S}\times\mathcal{S}\times\mathcal{A} \rightarrow [0,1]$ is the probability of transitioning to state $s_{t+1}$ when action $a_t$ is taken, given the current state is $s_t$.
%\begin{equation}
%p(s'\mid s,a)=P(s_t=s'\mid s_{t-1}=s, a_{t-1}=a) = \sum_{r\in\mathcal{R}}p(s',r \mid s,a)
%\end{equation}
Furthermore, we can calculate the expected reward for a given state-action pair $(s_t,a_t)$ as a function $r: \mathcal{S}\times\mathcal{A}\rightarrow \mathbb{R}$.
%\begin{equation}
%r(s_t,a_t)=\mathbb{E}[r_t\mid s_{t-1}=s,a_{t-1}=a]=\sum_{r\in\mathcal{R}}r\sum_{s'\in\mathcal{S}}p(s',r\mid s,a)
%\end{equation}

The state-value function, denoted by $V_{\pi}(s_t)$, represents the expected total discounted reward obtained when starting from state $s_t$ and following policy $\pi$. Similarly, the action-value function, denoted by $Q_{\pi}(s_t, a_t)$ is defined as the expected total discounted reward when starting in state $s_t$, taking action $a_t$ and then following the policy $\pi$. In this context, we refer to the action-value function as value function or Q-function.

\subsection{Soft Actor-Critic}
\label{sec:SAC}
Handling continuous state and action spaces with tabular RL algorithms can be computationally intensive and requires a significant amount of memory. We opted for the SAC algorithm to address this issue. This algorithm is widely used in RL and is designed to efficiently handle continuous state and action spaces. In actor-critic methods, the 'actor' models the policy $\pi(a_t|s_t)$, while the 'critic' models the value function $Q_\pi(s_t,a_t)$. SAC is a model-free RL algorithm that maximizes the maximum entropy objective, which aims to increase the policy's entropy while keeping the expected return constant:
\begin{equation}
	\pi^*=\argmax_{\pi}\sum_t \mathbb{E}_{(s_t,a_t)\sim\rho_{\pi}} \left [r(s_t,a_t)+\alpha \mathcal{H}(\pi(\cdot | s_t) \right ],
\end{equation}
with state-action marginal induced by the policy $\rho_{\pi}(s_t,a_t)$, temperature parameter $\alpha$ and entropy $ \mathcal{H}$.
The first part of this objective is equivalent to the classic maximizing total expected reward approach of RL algorithms, and the second term adds the maximum entropy of the policy. The temperature parameter controls the trade-off between these two. As $\alpha \rightarrow 0$, this objective is the same as in standard RL. The entropy term introduces stochasticity to the policy and thus favors exploration. 

SAC can be derived from the Soft Policy Iteration, an algorithm that alternates between the policy evaluation step, where the soft Q-function of the current policy $\pi$ is calculated, and the policy iteration step, where the policy is improved in terms of its soft Q-value. 
The set of possible policies $\Pi$ is constrained using the Kullback-Leibler (KL) divergence to ensure the tractability of the obtained policy.
For the tabular setting, the convergence of the Soft Policy Iteration towards the optimal policy $\pi^* \in \Pi$ in terms of the maximum entropy RL is guaranteed.

However, for continuous space and action dimensions, functional approximators are needed for the Q-value function $Q_{\theta}(s_t,a_t)$ and the policy $\pi_{\phi}(a_t|s_t)$ with parameters $\theta$ and $\phi$. For these, iterating the Soft Policy Iteration algorithm until convergence is computationally intractable. Hence, the authors introduced SAC, alternating between optimizing these networks with stochastic gradient descent. The parameters of the soft Q-value functions are minimized using the soft Bellman residual using a target network with parameters $\bar{\theta}$ to stabilize training:
\begin{equation}
	J_Q(\theta)=\mathbb{E}_{(s_t,a_t)\sim\mathcal{D}}\left[\frac{1}{2}(Q_\theta(s_t,a_t)-(r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim p}\left[V_{\bar{\theta}}(s_{t+1}) \right ]))^2 \right],
\end{equation}
using replay buffer data $\mathcal{D}$ and discount factor $\gamma$. The update rule given by stochastic gradient descent is denoted as $\hat{\triangledown}_{\theta}J_Q(\theta)$.
%The update rule given by stochastic gradient decent is given by 
%\begin{equation} \small
%	\hat{\triangledown}_{\theta}J_Q(\theta)=\triangledown_{\theta} Q_{\theta}(a_t,s_t)(Q_{\theta}(s_t,a_t)-(r(s_t,a_t)+\gamma(Q_{\bar{\theta}}(s_{t+1},a_{t+1})-\alpha \log (\pi_\phi(a_{t+1}|s_{t+1})))).
%\end{equation}

The objective function for updating the policy is obtained by minimizing the expected KL divergence: 
\begin{equation}
	J_{\pi}(\phi)=\mathbb{E}_{s_t\sim\mathcal{D},\epsilon_t\sim\mathcal{N}}\left [\alpha \log \pi_{\phi}(f_{\phi}(\epsilon_t;s_t)|s_t)-Q_{\theta}(s_t, f_{\phi}(\epsilon_t;s_t)) \right ],
\end{equation}
where the policy is reparametrizied by a neural network $a_t=f_{\phi}(\epsilon_t;s_t)$ with input noise $\epsilon_t$ to apply the reparametrization trick. This trick enables backpropagation through a sampling operation by transforming the sampling process  into a differentiable operation \cite{kingma2013auto}. The update rule is denoted as $\hat{\triangledown}_{\phi}J_{\pi}(\phi)$.
%\begin{equation}
%	\hat{\triangledown}_{\phi}J_{\pi}(\phi)=\triangledown_{\phi}\alpha\log (\pi_{\phi}(a_t|s_t))+(\triangledown_{a_t}\alpha \log (\pi_{\phi}(a_t|s_t))-\triangledown_{a_t}Q(s_t,a_t))\triangledown_{\phi}f_{\phi}(\epsilon_t;s_t).
%\end{equation}

The temperature parameter $\alpha$ is essential for the success of SAC and needs to be tuned for each task individually. Even during training of one single task, it would be advantageous if $\alpha$ would be adjustable: In regions where the policy is very sure about the best action, the entropy should be rather small and thus the policy deterministic. In regions where the policy is unsure, exploration should be enhanced. Thus, the authors presented a solution for adjusting the temperature parameter automatically and proved its theoretical optimality in the tabular setting. For this, the new objective considers the target minimum entropy $\mathcal{H}$ in a constraint to the classical maximum reward approach:
\begin{equation}
	\max_{\pi_{0:T}}\mathbb{E}_{\rho_{\pi}}\left[\sum_{t=0}^Tr(s_t,a_t) \right ] \textbf{ s.t. } \mathbb{E}_{(s_t,a_t)\sim \rho_{\pi}}\left [-\log(\pi_t(a_t|s_t)) \right ] \geq \mathcal{H}~\forall t
\end{equation}
After solving for the optimal soft Q-value function $Q^*_t$ and the optimal policy $\pi^*_t$, the optimal temperature parameter at time step $t$ is given by
\begin{equation}\label{eq:obj-alpha}
	\alpha^*_t = \argmin_{\alpha_t} \mathbb{E}_{a_t \sim \pi^*_t}\left [-\alpha_t \log \pi^*_t(a_t|s_t; \alpha_t)-\alpha_t \bar{\mathcal{H}} \right ].
\end{equation}
For the final SAC algorithm, as given in Algorithm \ref{alg:sac-update}, the authors propose to use two soft Q-function approximators trained independently to lower the positive bias introduced in the policy improvement step. The minimum value of these is used for updating both Q-function approximators as well as the policy. The temperature parameter $\alpha$ is learned by computing the gradient of the objective given in Equation \ref{eq:obj-alpha}, where $Q_t$ and $\pi_t$ are assumed to be optimal for the current time step.


\begin{algorithm}
\caption{Soft Actor-Critic \cite{haarnoja2018soft}}
\label{alg:sac-update}
\small\textbf{Input:}   $\theta_1,\theta_2,\phi$ \Comment{Initial parameters}
\begin{algorithmic}
\State $\bar{\theta}_1 \gets \theta_1, \bar{\theta}_2 \gets \theta_2$ \Comment{Initialize target network weights}
\State $\mathcal{D} \gets \emptyset$ \Comment{Initialize an empty replay pool}
\For{each iteration}
	\For{each environment step}
		\State $\mathbf{a}_t \sim \pi_{\phi}(\mathbf{a}_t|\mathbf{s}_t)$ \Comment{Sample action from the policy}
		\State $\mathbf{s}_{t+1} \sim p(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)$ \Comment{Sample transition from the environment}
		\State $\mathcal{D} \gets \mathcal{D} \cup \{(\mathbf{s}_t,\mathbf{a}_t,r(\mathbf{s}_t,\mathbf{a}_t),\mathbf{s}_{t+1})\}$ \Comment{Store the transition in the replay pool}
	\EndFor
	\For{each gradient step}
		\State $\theta_i \gets \theta_i - \lambda_Q \hat{\triangledown}_{\theta_i}J_Q(\theta_i)$ for $i \in \{1,2\}$ \Comment{Update Q-function parameters}
		\State $\phi \gets \phi - \lambda _{\pi} \hat{\triangledown}_{\phi}J_{\pi}(\phi)$ \Comment{Update policy weights}
		\State $\alpha \gets \alpha - \lambda  \hat{\triangledown}_{\alpha}J(\alpha)$ \Comment{Adjust temperature}
		\State $\bar{\theta}_i \gets \tau\theta_i + (1-\tau)\bar{\theta}_i$ for $i \in \{1,2\}$ \Comment{Update target network weights}
	\EndFor
\EndFor
\end{algorithmic}
\textbf{Output:}   $\theta_1,\theta_2,\phi$ \Comment{Optimized parameters}
\end{algorithm}


\input{Background}
\cleardoublepage

%% 
\input{MM}
\cleardoublepage

%%
\input{results}
Used PV of available\\
$possible\_battery\_input_{b,t} = \min(\min(C^b_t-SOC^b_t, 0), max\_input\_power_{b, \textcolor{red}{t=last}})$\\
\\
$could\_used\_without\_pv_{b,t} = possible\_battery\_input_{b,t} + net\_electricity\_consumption\_without\_storage\_and\_pv_{b,t} $ \\
\\
$could\_used\_with\_pv_{b,t} = \max(possible\_battery\_input_{b,t} + net\_electricity\_consumption\_without\_storage\_and\_pv_{b,t} - solar\_generation_{b,t}*-1, 0)$ \\
\\
$pv\_could\_have\_used_{b,t} = \min(solar\_generation_{b,t}*-1, could\_used\_without\_pv_{b,t})$ \\
\\
$pv\_used_{b,t} = E_{used_{pv_{b,t}}} = \max(\min(e_{net}^b - e_{pv}^b, - e_{pv}^b), 0) $ \\
\\
$pv\_used\_of\_available_{b,t} = used_{b,t}/pv\_could\_have\_used_{b,t}$\\
\\
$grid\_could\_have\_used_{t} = \min(E_{r,grid_{t}}, \sum_{b\in Buildings}could\_used\_with\_pv_{b,t})$ \\
\\
$grid\_used_{t} = E_{used_{r,grid_{t}}} = \min(E_{net_{pos}}, E_{r, grid})$ \\
\\
$grid\_used\_of\_available_{t} = grid\_used_{b,t} / grid\_could\_have\_used_{b,t}$ \\
\\
$total\_could\_have\_used_{t} = \min(E_{r,grid_{t}} + \sum_{b\in Buildings} \min(solar\_generation_{b,t}*-1, could\_used\_without\_pv_{b,t}), \sum_{b\in Buildings}could\_used\_with\_pv_{b,t}))$ \\
\\
$total\_used_{t} = E_{used_{r}} = E_{used_{r,grid}}  + E_{used_{pv}}$ \\
\\
$total\_used\_of\_available_{t} = total\_used_{t} / total\_could\_have\_used_{t}$


\cleardoublepage

%%
\input{discussion}
\cleardoublepage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

%\setcounter{secnumdepth}{-1}
%\section{Tables}\label{chap:App}
\chapter{Appendix}\label{chap:App}
\todo[inline]{only relevant ones?}
\begin{table}[htb]
\begin{tabularx}{\linewidth}{lX}
name & description \\ \hline
month & 1 (January) through 12 (December)\\
 day & type of day as provided by EnergyPlus (from 1 to 8). 1 (Sunday), 2 (Monday), ..., 7 (Saturday), 8 (Holiday)\\
 hour & hour of day (from 1 to 24)\\
 daylight\_savings\_status & indicates if the building is under daylight savings period (0 to 1). 0 indicates that the building has not changed its electricity consumption profiles due to daylight savings, while 1 indicates the period in which the building may have been affected.\\
 t\_out & outdoor temperature in Celcius degrees.\\
 t\_out\_pred\_6h & outdoor temperature predicted 6h ahead \\
 t\_out\_pred\_12h & outdoor temperature predicted 12h ahead \\
 t\_out\_pred\_24h & outdoor temperature predicted 24h ahead \\
 rh\_out & outdoor relative humidity in \%.\\
 rh\_out\_pred\_6h & outdoor relative humidity predicted 6h ahead \_\\
 rh\_out\_pred\_12h & outdoor relative humidity predicted 12h ahead \_\\
 rh\_out\_pred\_24h & outdoor relative humidity predicted 24h ahead \_\\
 diffuse\_solar\_rad & diffuse solar radiation in $W/m^2$.\\
 diffuse\_solar\_rad\_pred\_6h & diffuse solar radiation predicted 6h ahead \_\\
 diffuse\_solar\_rad\_pred\_12h & diffuse solar radiation predicted 12h ahead \_\\
 diffuse\_solar\_rad\_pred\_24h & diffuse solar radiation predicted 24h ahead \_\\
 direct\_solar\_rad & direct solar radiation in $W/m^2$. \\
 direct\_solar\_rad\_pred\_6h & direct solar radiation predicted 6h ahead \_\\ 
 direct\_solar\_rad\_pred\_12h & direct solar radiation predicted 12h ahead \_\\ 
 direct\_solar\_rad\_pred\_24h & direct solar radiation predicted 24h ahead \_\\ 
 t\_in & indoor temperature in Celcius degrees.\\ 
 avg\_unmet\_setpoint & average difference between the indoor temperatures and the cooling temperature setpoints in the different zones of the building in Celcius degrees. $sum((t\_in, t\_setpoint).clip(min=0) * zone\_volumes)/total\_volume$\\ 
 rh\_in & indoor relative humidity in \%.\\ 
 non\_shiftable\_load & electricity currently consumed by electrical appliances in kWh.\\ 
 solar\_gen & electricity currently being generated by photovoltaic panels in kWh.\\ 
 cooling\_storage\_soc & state of the charge (SOC) of the cooling storage device. From 0 (no energy stored) to 1 (at full capacity).\\ 
 dhw\_storage\_soc & state of the charge (SOC) of the domestic hot water (DHW) storage device. From 0 (no energy stored) to 1 (at full capacity).\\ 
 net\_electricity\_consumption & net electricity consumption of the building (including all energy systems) in the current time step.
\end{tabularx}
\caption[States of the CityLearn Framework]{States of the CityLearn Framework \textcolor{red}{unvollständig} \cite{vazquez2020citylearn}}\label{tab:citylearn-states}
\end{table}

\begin{table}[htb]
\begin{tabularx}{\linewidth}{lX}
name & description \\ \hline
month & 1 (January) through 12 (December)\\
day\_type & type of day as provided by EnergyPlus (from 1 to 8). 1 (Sunday), 2 (Monday), ..., 7 (Saturday), 8 (Holiday)\\
hour & hour of day (from 1 to 24)\\
 t\_out & outdoor temperature in Celcius degrees.\\
 t\_out\_pred\_6h & outdoor temperature predicted 6h ahead \textcolor{red}{(\nolinebreak accuracy: +-0.3C)}\\
 t\_out\_pred\_12h & outdoor temperature predicted 12h ahead \textcolor{red}{(\nolinebreak accuracy: +-0.65C)}\\
 t\_out\_pred\_24h & outdoor temperature predicted 24h ahead \textcolor{red}{(\nolinebreak accuracy: +-1.35C)}\\
 rh\_out & outdoor relative humidity in \%.\\
 rh\_out\_pred\_6h & outdoor relative humidity predicted 6h ahead \\
 rh\_out\_pred\_12h & outdoor relative humidity predicted 12h ahead \\
 rh\_out\_pred\_24h & outdoor relative humidity predicted 24h ahead \\
 diffuse\_solar\_rad & diffuse solar radiation in $W/m^2$.\\
 diffuse\_solar\_rad\_pred\_6h & diffuse solar radiation predicted 6h ahead \\
 diffuse\_solar\_rad\_pred\_12h & diffuse solar radiation predicted 12h ahead \\
 diffuse\_solar\_rad\_pred\_24h & diffuse solar radiation predicted 24h ahead \\
 direct\_solar\_rad & direct solar radiation in $W/m^2$. \\
 direct\_solar\_rad\_pred\_6h & direct solar radiation predicted 6h ahead \\ 
 direct\_solar\_rad\_pred\_12h & direct solar radiation predicted 12h ahead \\ 
 direct\_solar\_rad\_pred\_24h & direct solar radiation predicted 24h ahead \\ 
 wind\_speed & wind speed in $m/s$ \\
 wind\_speed\_pred\_6h & wind speed predicted 6h ahead \\
 wind\_speed\_pred\_12h & wind speed predicted 12h ahead \\
 wind\_speed\_pred\_24h & wind speed predicted 24h ahead \\
 non\_shiftable\_load & electricity currently consumed by electrical appliances in kWh.\\ 
 solar\_gen & electricity currently being generated by photovoltaic panels in kWh.\\ 
 electrical\_storage\_soc & SOC of the electrical storage from 0 (no energy stored) to 1 (at full capacity). \\
 net\_electricity\_consumption & net electricity consumption of the building (including all energy systems) in the current time step. \\
 electricity\_pricing & Electricity rate in $\$/kWh$ \\
 electricity\_pricing\_pred\_6h & Electricity rate predicted 6 hours ahead. \\
 electricity\_pricing\_pred\_12h & Electricity rate predicted 12 hours ahead. \\
 electricity\_pricing\_pred\_24h & Electricity rate predicted 24 hours ahead. \\
 renewable\_energy\_share & Share of renewable energy in the fuel mix of the grid.
\end{tabularx}
\caption[Observation space of the baseline SAC Agent]{Observation space of the baseline SAC Agent}\label{tab:citylearn-states}
\label{app:observation-space-sac}
\end{table}

%\chapter{Figures}\label{chap:App2}

\begin{table}[htb]
\begin{tabularx}{\linewidth}{lX}
Attribute Name & Description \\  \hline
Month & 1\\
Hour & 2\\ 
Day Type & 3\\
Daylight Saving Status & 4\\
Equipment Electric Power in kWh& 5\\
Solar Generation in W/kW
\end{tabularx}
\caption{\textcolor{red}{Zweite Appendix-Tabelle}}
\label{tab:buildings-vars}
\end{table}

\begin{table}[htb]
\begin{tabularx}{\linewidth}{lXXXXXXX}
Id & Demonstrator & Energy Consumption Median & Energy Consumption without storage Median & Energy Consumption without storage and PV Median \\  \hline
1 & 0  & 0.54 & 0.58 & 0.81 \\
2 & 0  & 0.45 & 0.49 & 0.74 \\
3 & 0  & 0.25 & 0.28 & 0.56 \\
4 & 0  & 0.52 & 0.54 & 0.94  \\
5 & 0  & 0.29 & 0.29 & 0.74 \\
6 & 0  & 0.5 & 0.56 & 1.08 \\
7 & 0  & 0.21 & 0.21 & 0.33 \\
8 & 0  & 0.31 & 0.32 & 0.68 \\
9 & 0  & 0.36 & 0.38 & 0.46 \\
10 & 0  & 0.56 & 0.65 & 0.98 \\
11 & 0  & 0.66 & 0.71 & 1.19 \\
12 & 0  & 0.0 & 0.0 & 0.0 \\
13 & 0  & 0.43 & 0.48 & 0.97 \\
14 & 0  & 0.49 & 0.48 & 0.59 \\
15 & 0  & 0.12 & 0.0 & 0.0 \\
16 & 0  & 0.55 & 0.72 & 1.2 \\
17 & 0  & 0.75 & 0.82 & 1.24 \\
\end{tabularx}
\caption{\textcolor{red}{Zweite Appendix-Tabelle}}
\label{tab:building-medians}
\end{table}
%\end{appendices)

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Bibliographie
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\addcontentsline{toc}{chapter}{Bibliography}

\bibliographystyle{alpha}
\bibliography{thesislit}
%% Obige Anweisung legt fest, dass BibTeX-Datei `mylit.bib' verwendet
%% wird. Hier koennen mehrere Dateinamen mit Kommata getrennt aufgelistet
%% werden.

\cleardoublepage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Erklaerung
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{empty}
\section*{Selbst\"andigkeitserkl\"arung}

Hiermit versichere ich, dass ich die vorliegende Masterarbeit 
selbst\"andig und nur mit den angegebenen Hilfsmitteln angefertigt habe und dass alle Stellen, die dem Wortlaut oder dem 
Sinne nach anderen Werken entnommen sind, durch Angaben von Quellen als 
Entlehnung kenntlich gemacht worden sind. 
Diese Masterarbeit wurde in gleicher oder \"ahnlicher Form in keinem anderen 
Studiengang als Pr\"ufungsleistung vorgelegt. 

\vskip 3cm

Ort, Datum	\hfill Unterschrift \hfill 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Ende
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

