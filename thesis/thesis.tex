%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% LaTeX-Rahmen fuer das Erstellen von Masterarbeiten
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% allgemeine Einstellungen
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[twoside,12pt,a4paper]{report}
%\usepackage{reportpage}
\usepackage{epsf}
\usepackage{graphics, graphicx}
\usepackage{latexsym}
\usepackage[margin=10pt,font=small,labelfont=bf]{caption}
\usepackage[utf8]{inputenc}
\usepackage[toc,page]{appendix}


% Own packages
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}

\textwidth 14cm
\textheight 22cm
\topmargin 0.0cm
\evensidemargin 1cm
\oddsidemargin 1cm
%\footskip 2cm
\parskip0.5explus0.1exminus0.1ex

% Kann von Student auch nach pers\"onlichem Geschmack ver\"andert werden.
\pagestyle{headings}

\sloppy

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% hier steht die neue Titelseite 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
\begin{titlepage}
 \begin{center}
  {\LARGE Eberhard Karls Universit\"at T\"ubingen}\\
  {\large Mathematisch-Naturwissenschaftliche Fakult\"at \\
Wilhelm-Schickard-Institut f\"ur Informatik\\[4cm]}
  {\huge Master Thesis Computer Science\\[2cm]}
  {\Large\bf  Incorporating Social Learning into Multi-Agent Reinforcement Learning
to \\Lower Carbon Emissions in Energy Systems\\[1.5cm]}
 {\large Kristina Lietz}\\[0.5cm]
01.12.2023\\[4cm]
{\small\bf Reviewers}\\[0.5cm]
  \parbox{7cm}{\begin{center}{\large Dr. Nicole Ludwig}\\
   (Bioinformatik)\\
  {\footnotesize Wilhelm-Schickard-Institut f\"ur Informatik\\
	Universit\"at T\"ubingen}\end{center}}\hfill\parbox{7cm}{\begin{center}
  {\large Prof. Setareh Maghsudit}\\
  (Biologie/Medizin)\\
  {\footnotesize Medizinische Fakult\"at\\
	Universit\"at T\"ubingen}\end{center}
 }
  \end{center}
\end{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Titelr"uckseite: Bibliographische Angaben
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thispagestyle{empty}
\vspace*{\fill}
\begin{minipage}{11.2cm}
\textbf{Lietz, Kristina:}\\
\emph{Incorporating Social Learning into Multi-Agent Reinforcement Learning
to Lower Carbon Emissions in Energy Systems}\\ Master Thesis Computer Science\\
Eberhard Karls Universit\"at T\"ubingen\\
Thesis period: 01.06.2023~-~01.12.2023
\end{minipage}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagenumbering{roman}
\setcounter{page}{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Seite I: Zusammenfassug, Danksagung
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section*{Abstract}

\todo[inline]{Write here your abstract.}

\newpage
\section*{Zusammenfassung}

\todo[inline]{Bei einer englischen Masterarbeit muss zus\"atzlich eine deutsche Zusammenfassung verfasst werden.}

\newpage
\section*{Acknowledgements}

\todo[inline]{Write here your acknowledgements.}

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Table of Contents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\baselinestretch}{1.3}
\small\normalsize

\tableofcontents

\renewcommand{\baselinestretch}{1}
\small\normalsize

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% List of Figures
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\baselinestretch}{1.3}
\small\normalsize

\addcontentsline{toc}{chapter}{List of Figures}
\listoffigures

\renewcommand{\baselinestretch}{1}
\small\normalsize

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% List of tables
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\baselinestretch}{1.3}
\small\normalsize

\addcontentsline{toc}{chapter}{List of Tables}
\listoftables

\renewcommand{\baselinestretch}{1}
\small\normalsize

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% List of abbreviations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% can be removed
\addcontentsline{toc}{chapter}{List of Abbreviations}
\chapter*{List of Abbreviations\markboth{LIST OF ABBREVIATIONS}{LIST OF ABBREVIATIONS}}
\todo[inline]{TODO}
\begin{tabbing}
\textbf{FACTOTUM}\hspace{1cm}\=Schrott\kill
\textbf{MDP}\>Markov decision process \\
\textbf{RL}\>Reinforcement learning \\
\textbf{SAC}\>Soft actor-critic \\
\textbf{...} \> ...\\
\end{tabbing}

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Der Haupttext, ab hier mit arabischer Numerierung
%%% Mit \input{dateiname} werden die Datei `dateiname' eingebunden
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagenumbering{arabic}
\setcounter{page}{1}

%% Introduction
\input{intro}
\begin{figure}[htb]
     \centerline{\epsffile{figures/chordal.eps}}
  \caption{Chordale Graphen}
  \label{fig2.1}
\end{figure}

Abbildung~\ref{fig2.1} zeigt ...

{
\renewcommand{\baselinestretch}{0.9} 
\normalsize
\begin{table}[htb]
\begin{tabular}{|p{2.7cm}||l|c|r|}
\hline
    \textbf{Spalte 1} 
  & \textbf{Spalte 2} 
  & \textbf{Spalte 3} 
  & \textbf{Spalte 4} \\
  \hline\hline
  xxx1111
  & xxxxxxx2222222
  & xxxxxx333333 
  & xxxxxxxxxx444444 \\
  \hline
    ...
  & ...
  & ...
  & ...\\
  \hline
\end{tabular}
  \caption[Beispieltabelle mit einer langen Legende]{Beispieltabelle mit einer langen Legende, damit man sieht, dass in der Legende der Zeilenabstand verringert wurde. Ausserdem soll auch der Font etwas kleiner gew\"ahlt werden. So sieht die ganze Umgebung kompakter aus.}
  \label{tabelle-1}
\end{table}
}
Referenzen: \cite{SaaSchTue97,TueConSaa96ismis,SchTueSaa98preprint}

\cleardoublepage

%% 
\chapter{Background}
\label{Background}
This chapter provides the necessary background knowledge to comprehend our approach to the presented problem. First, we introduce the fundamental concepts of reinforcement learning (RL), often modeled as finite Markov decision processes (MDP). However, considering the limitations of this traditional framework in handling continuous state and action spaces, we also delve into the soft actor-critic method as a promising solution.
\todo[inline]{TODO}

\section{Reinforcement Learning}
As defined by Sutton and Barto \cite{sutton2018reinforcement}, RL is a machine learning method that aims to choose actions to maximize a numerical reward. RL agents learn these actions through a trial-and-error approach based on the received rewards. Also, actions may, besides the immediate reward, influence the long-term reward. In contrast to supervised methods, it does not need labeled data for learning and does not try to discover inherent structure in data as unsupervised methods.

\begin{figure}[htb]
     \center
     \includegraphics[width=\textwidth]{figures/rl_overview.pdf}
  \caption{In RL, the agent observes the state of the environment $s_t$ at timestep $t$ via the observation $o_t$. The policy $\pi$ determines the agent's action $a_t$, which in turn determines the next state $s_{t+1}$ of the environment and the environment sends an reward $r_t$ to the agent. The policy of the agent is trained using the observation, the action, and the reward,}
  \label{fig:rl_overview}
\end{figure}

\noindent
Figure \ref{fig:rl_overview} shows the general RL approach. RL problems are often formulated as finite MDPs, 
%defined by a tuple $(\mathcal{S}, \mathcal{A}, p, r)$, 
where the agent interacts with the environment over a series of time steps. At each time step $t$, the agent observes the current state of the environment, denoted as $s_t \in \mathcal{S}$. Based on this state, the agent selects an action $a_t \in \mathcal{A}$. We assume, that the set of possible actions $\mathcal{A}$ is the same in all states. As a consequence of the chosen action, the agent receives at the next time step a numerical reward $r_{t+1}\in\mathcal{R} \subset \mathbb{R}$ and a new state $s_{t+1}$.

\todo[inline]{discrete spaces, markov property}

The dynamics of the MDP are captured by a function $p: \mathcal{S}\times\mathcal{R}\times\mathcal{S}\times\mathcal{A} \rightarrow [0,1]$. This function represents the probability of transitioning from state $s\in\mathcal{S}$ to state $s'\in\mathcal{S}$ and receiving reward $r\in\mathcal{R}$ when the action $a\in\mathcal{A}$ is taken:
\begin{equation}
p(s', r \mid s,a)=P(s_t=s', r_t=r \mid s_{t-1}=s, a_{t-1}=a)
\end{equation}
Using the dynamics function $p$, we can calculate additional quantities of interest. The state-transition probability defined as function $p: \mathcal{S}\times\mathcal{S}\times\mathcal{A} \rightarrow [0,1]$ is the probability of transitioning to state $s'$ when action $a$ is taken, given the current state is $s$: 
\begin{equation}
p(s'\mid s,a)=P(s_t=s'\mid s_{t-1}=s, a_{t-1}=a) = \sum_{r\in\mathcal{R}}p(s',r \mid s,a)
\end{equation}
Furthermore, we can calculate the expected reward for a given state-action pair $(s,a)$ as function $r: \mathcal{S}\times\mathcal{A}\rightarrow \mathbb{R}$: 
\begin{equation}
r(s,a)=\mathbb{E}[r_t\mid s_{t-1}=s,a_{t-1}=a]=\sum_{r\in\mathcal{R}}r\sum_{s'\in\mathcal{S}}p(s',r\mid s,a)
\end{equation}

\begin{itemize}
\item Goal: maximize expected discounted return $G_t = R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+... = \sum_{k=0}^{\infty}\gamma^kR_{t+k+1}$ with discount rate $0 \leq \gamma \leq 1$
\end{itemize} 


The policy $\pi: \mathcal{S}=p(a_t|s_t)$ determines the action, which may also be deterministic. Also, reacting to the received action, the environment sends the agent a reward signal $r_t$. The agent's policy is trained to maximize the discounted total reward. 


\todo[inline]{TODO Continuous action - 335, continuous state ---$>$ function approximation}
\input{Background}
\cleardoublepage

%% 
\input{MM}
\cleardoublepage

%%
\input{results}
\cleardoublepage

%%
\input{discussion}
\cleardoublepage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

%\setcounter{secnumdepth}{-1}
%\section{Tables}\label{chap:App}
\chapter{Appendix}\label{chap:App}
Viele Arbeiten haben einen Appendix. Besondere Sorgfalt muss beim Nummerieren der Tabellen und Abbildungen gew√§hrleistet sein.
\begin{table}[htb]
\begin{tabular}{cc}
Nummer & Datum \\
\hline
1 & 1.1.80\\
2 & 1.1.90 \\
\end{tabular}
\caption{Erste Appendix-Tabelle}\label{tab:app1}
\end{table}

%\chapter{Figures}\label{chap:App2}

\begin{table}[htb]
\begin{tabular}{cc}
Nummer & Datum \\
\hline
1 & 1.1.80\\
2 & 1.1.90 \\
\end{tabular}
\caption{Zweite Appendix-Tabelle}\label{tab:app2}
\end{table}
%\end{appendices)

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Bibliographie
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\addcontentsline{toc}{chapter}{Bibliography}

\bibliographystyle{alpha}
\bibliography{thesislit}
%% Obige Anweisung legt fest, dass BibTeX-Datei `mylit.bib' verwendet
%% wird. Hier koennen mehrere Dateinamen mit Kommata getrennt aufgelistet
%% werden.

\cleardoublepage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Erklaerung
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{empty}
\section*{Selbst\"andigkeitserkl\"arung}

Hiermit versichere ich, dass ich die vorliegende Masterarbeit 
selbst\"andig und nur mit den angegebenen Hilfsmitteln angefertigt habe und dass alle Stellen, die dem Wortlaut oder dem 
Sinne nach anderen Werken entnommen sind, durch Angaben von Quellen als 
Entlehnung kenntlich gemacht worden sind. 
Diese Masterarbeit wurde in gleicher oder \"ahnlicher Form in keinem anderen 
Studiengang als Pr\"ufungsleistung vorgelegt. 

\vskip 3cm

Ort, Datum	\hfill Unterschrift \hfill 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Ende
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

